{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fbf2be2",
   "metadata": {},
   "source": [
    "# Data pre-process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce4f089-bb5f-4fa5-a16a-0da82c83c362",
   "metadata": {},
   "source": [
    "### Imports and paths initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "2c024d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the whole notebook\n",
    "import csv\n",
    "import random\n",
    "import warnings\n",
    "import os\n",
    "import argparse\n",
    "import re\n",
    "from datetime import datetime\n",
    "from xml.etree import ElementTree as ET\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rdflib\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import KeyedVectors\n",
    "import nltk\n",
    "from owl2vec_star import owl2vec_star\n",
    "from sdv import SDV\n",
    "from sdv.evaluation import evaluate\n",
    "from sdv.metrics.tabular import BinaryDecisionTreeClassifier, BinaryAdaBoostClassifier,\\\n",
    "    BinaryLogisticRegression, BinaryMLPClassifier, BNLikelihood, LogisticDetection,\\\n",
    "    CSTest, KSTest\n",
    "\n",
    "\n",
    "def parse_args(args=None):\n",
    "    \"\"\"Finds a sub-tag of a source tag and inputs its value into a dictionary\n",
    "        containing the current row's data\n",
    "\n",
    "    Args:\n",
    "        args (list-like):\n",
    "            List of arguments to set. Uses default values if no args are passed.\n",
    "    Returns:\n",
    "        (argparse.ArgumentParser):\n",
    "            Parsed arguments object.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Data pre-process',\n",
    "        usage=''\n",
    "    )\n",
    "    parser.add_argument('--data_dir', type=str, default='data')\n",
    "    parser.add_argument('--onto_dir', type=str, default='ontology')\n",
    "    parser.add_argument('--syn_data_dir', type=str, default='syn_data')\n",
    "    parser.add_argument('--onto_to_embed', type=str, default='hpObo_hoom_ordo.owl')\n",
    "    parser.add_argument('--embedding_cfg_path', type=str, default='./embedding.cfg')\n",
    "\n",
    "    return parser.parse_args(args)\n",
    "\n",
    "\n",
    "# set the path to your data folders here\n",
    "args = parse_args(args=['--data_dir', '../persistent/data'])\n",
    "onto_dir_path = os.path.join(args.data_dir, args.onto_dir)\n",
    "syn_data_dir_path = os.path.join(args.data_dir, args.syn_data_dir)\n",
    "\n",
    "if not os.path.exists(onto_dir_path):\n",
    "    raise ValueError(f'You need an existing ontology directory with the XML dataset inside it, please create \\'{onto_dir_path}\\'')\n",
    "\n",
    "if not os.path.exists(syn_data_dir_path):\n",
    "    os.makedirs(syn_data_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606da578",
   "metadata": {},
   "source": [
    "### Convert XML dataset to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1344a509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: clinical signs and symptoms in rare diseases\n",
    "# http://www.orphadata.org/cgi-bin/index.php (Phenotypes associated with rare disorders)\n",
    "\n",
    "\n",
    "tree = ET.parse(os.path.join(onto_dir_path, 'en_product4.xml'))\n",
    "root = tree.getroot()\n",
    "\n",
    "\n",
    "headers = ['HPODisorderSetStatus_id', 'Disorder_id', 'OrphaCode', 'ExpertLink',\n",
    "           'Name', 'DisorderType_id', 'DisorderType_name', 'DisorderGroup_id',\n",
    "           'DisorderGroup_Name', 'HPODisorderAssociation_id', 'HPO_id',\n",
    "           'HPOId', 'HPOTerm', 'HPOFrequency_id', 'HPOFrequency_Name',\n",
    "           'DiagnosticCriteria_id', 'DiagnosticCriteria_Name', 'Source',\n",
    "           'ValidationStatus', 'Online', 'ValidationDate']\n",
    "\n",
    "\n",
    "def find_value(row_data, source_tag, target_tag_name, field, text=True):\n",
    "    \"\"\"Finds a sub-tag of a source tag and inputs its value into a dictionary\n",
    "        containing the current row's data\n",
    "\n",
    "    Args:\n",
    "        row_data (dict):\n",
    "            The data for the current row associated with the csv fields\n",
    "        source_tag (Element):\n",
    "            XML parent tag to search from\n",
    "        target_tag_name (str):\n",
    "            Name of the sub-tag to find\n",
    "        field (str):\n",
    "            Field in the csv file\n",
    "        text (bool):\n",
    "            Indicates if the value of the tag to retrieve is its inner text \n",
    "            or its id attribute\n",
    "    Returns:\n",
    "        tag (Element):\n",
    "            Returns the found tag\n",
    "    \"\"\"\n",
    "    tag = source_tag.find(target_tag_name)\n",
    "    tag_v = ''\n",
    "\n",
    "    if tag is not None:  # retrieving either the inner text or the id attribute of the tag\n",
    "        if text:\n",
    "            tag_v = tag.text\n",
    "        elif (len(tag.attrib) > 0):\n",
    "            tag_v = tag.attrib['id']\n",
    "    row_data[field] = tag_v if tag_v is not None else ''\n",
    "\n",
    "    return tag\n",
    "\n",
    "\n",
    "with open(os.path.join(onto_dir_path, 'en_product4.csv'), 'w', encoding='utf-8') as fd:\n",
    "    csvwriter = csv.DictWriter(fd, delimiter=',', fieldnames=headers)\n",
    "    csvwriter.writeheader()\n",
    "\n",
    "    # iterating through all the disorders\n",
    "    for status in root.find('HPODisorderSetStatusList').findall('HPODisorderSetStatus'):\n",
    "        row_data = {}\n",
    "        row_data['HPODisorderSetStatus_id'] = status.attrib['id']\n",
    "\n",
    "        disorder_tag = find_value(row_data, status, 'Disorder', 'Disorder_id', text=False)\n",
    "        find_value(row_data, disorder_tag, 'OrphaCode', 'OrphaCode', text=True)\n",
    "        find_value(row_data, disorder_tag, 'ExpertLink', 'ExpertLink', text=True)\n",
    "        find_value(row_data, disorder_tag, 'Name', 'Name', text=True)\n",
    "\n",
    "        disordertype_tag = find_value(row_data, disorder_tag, 'DisorderType', 'DisorderType_id', text=False)\n",
    "        find_value(row_data, disordertype_tag, 'Name', 'DisorderType_name', text=True)\n",
    "        disordergroup_tag = find_value(row_data, disorder_tag, 'DisorderGroup', 'DisorderGroup_id', text=False)\n",
    "        find_value(row_data, disordergroup_tag, 'Name', 'DisorderGroup_Name', text=True)\n",
    "\n",
    "        for field in ['Source', 'ValidationStatus', 'Online', 'ValidationDate']:\n",
    "            find_value(row_data, status, field, field, text=True)\n",
    "\n",
    "        # iterating through all the disorder associations and writing a row for each association\n",
    "        for association in disorder_tag.find('HPODisorderAssociationList').findall('HPODisorderAssociation'):\n",
    "            row_data['HPODisorderAssociation_id'] = association.attrib['id']\n",
    "\n",
    "            hpo_tag = find_value(row_data, association, 'HPO', 'HPO_id', text=False)\n",
    "            find_value(row_data, hpo_tag, 'HPOId', 'HPOId', text=True)\n",
    "            find_value(row_data, hpo_tag, 'HPOTerm', 'HPOTerm', text=True)\n",
    "            hpofrequency_tag = find_value(row_data, association, 'HPOFrequency', 'HPOFrequency_id', text=False)\n",
    "            find_value(row_data, hpofrequency_tag, 'Name', 'HPOFrequency_Name', text=True)\n",
    "\n",
    "            diagnosticcriteria_tag = find_value(row_data, association, 'DiagnosticCriteria', 'DiagnosticCriteria_id', text=False)\n",
    "            find_value(row_data, diagnosticcriteria_tag, 'Name', 'DiagnosticCriteria_Name', text=True)\n",
    "\n",
    "            csvwriter.writerow(row_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ebdae01d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HPODisorderSetStatus_id</th>\n",
       "      <th>Disorder_id</th>\n",
       "      <th>OrphaCode</th>\n",
       "      <th>ExpertLink</th>\n",
       "      <th>Name</th>\n",
       "      <th>DisorderType_id</th>\n",
       "      <th>DisorderType_name</th>\n",
       "      <th>DisorderGroup_id</th>\n",
       "      <th>DisorderGroup_Name</th>\n",
       "      <th>HPODisorderAssociation_id</th>\n",
       "      <th>...</th>\n",
       "      <th>HPOId</th>\n",
       "      <th>HPOTerm</th>\n",
       "      <th>HPOFrequency_id</th>\n",
       "      <th>HPOFrequency_Name</th>\n",
       "      <th>DiagnosticCriteria_id</th>\n",
       "      <th>DiagnosticCriteria_Name</th>\n",
       "      <th>Source</th>\n",
       "      <th>ValidationStatus</th>\n",
       "      <th>Online</th>\n",
       "      <th>ValidationDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>http://www.orpha.net/consor/cgi-bin/OC_Exp.php...</td>\n",
       "      <td>Alexander disease</td>\n",
       "      <td>21394</td>\n",
       "      <td>Disease</td>\n",
       "      <td>36547</td>\n",
       "      <td>Disorder</td>\n",
       "      <td>327485</td>\n",
       "      <td>...</td>\n",
       "      <td>HP:0000256</td>\n",
       "      <td>Macrocephaly</td>\n",
       "      <td>28412</td>\n",
       "      <td>Very frequent (99-80%)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>2016-06-01 00:00:00.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>http://www.orpha.net/consor/cgi-bin/OC_Exp.php...</td>\n",
       "      <td>Alexander disease</td>\n",
       "      <td>21394</td>\n",
       "      <td>Disease</td>\n",
       "      <td>36547</td>\n",
       "      <td>Disorder</td>\n",
       "      <td>327486</td>\n",
       "      <td>...</td>\n",
       "      <td>HP:0001249</td>\n",
       "      <td>Intellectual disability</td>\n",
       "      <td>28412</td>\n",
       "      <td>Very frequent (99-80%)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>2016-06-01 00:00:00.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>http://www.orpha.net/consor/cgi-bin/OC_Exp.php...</td>\n",
       "      <td>Alexander disease</td>\n",
       "      <td>21394</td>\n",
       "      <td>Disease</td>\n",
       "      <td>36547</td>\n",
       "      <td>Disorder</td>\n",
       "      <td>327487</td>\n",
       "      <td>...</td>\n",
       "      <td>HP:0001250</td>\n",
       "      <td>Seizures</td>\n",
       "      <td>28412</td>\n",
       "      <td>Very frequent (99-80%)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>2016-06-01 00:00:00.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>http://www.orpha.net/consor/cgi-bin/OC_Exp.php...</td>\n",
       "      <td>Alexander disease</td>\n",
       "      <td>21394</td>\n",
       "      <td>Disease</td>\n",
       "      <td>36547</td>\n",
       "      <td>Disorder</td>\n",
       "      <td>327488</td>\n",
       "      <td>...</td>\n",
       "      <td>HP:0001257</td>\n",
       "      <td>Spasticity</td>\n",
       "      <td>28412</td>\n",
       "      <td>Very frequent (99-80%)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>2016-06-01 00:00:00.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>http://www.orpha.net/consor/cgi-bin/OC_Exp.php...</td>\n",
       "      <td>Alexander disease</td>\n",
       "      <td>21394</td>\n",
       "      <td>Disease</td>\n",
       "      <td>36547</td>\n",
       "      <td>Disorder</td>\n",
       "      <td>327489</td>\n",
       "      <td>...</td>\n",
       "      <td>HP:0001274</td>\n",
       "      <td>Agenesis of corpus callosum</td>\n",
       "      <td>28412</td>\n",
       "      <td>Very frequent (99-80%)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>2016-06-01 00:00:00.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   HPODisorderSetStatus_id  Disorder_id  OrphaCode  \\\n",
       "0                        1            2         58   \n",
       "1                        1            2         58   \n",
       "2                        1            2         58   \n",
       "3                        1            2         58   \n",
       "4                        1            2         58   \n",
       "\n",
       "                                          ExpertLink               Name  \\\n",
       "0  http://www.orpha.net/consor/cgi-bin/OC_Exp.php...  Alexander disease   \n",
       "1  http://www.orpha.net/consor/cgi-bin/OC_Exp.php...  Alexander disease   \n",
       "2  http://www.orpha.net/consor/cgi-bin/OC_Exp.php...  Alexander disease   \n",
       "3  http://www.orpha.net/consor/cgi-bin/OC_Exp.php...  Alexander disease   \n",
       "4  http://www.orpha.net/consor/cgi-bin/OC_Exp.php...  Alexander disease   \n",
       "\n",
       "   DisorderType_id DisorderType_name  DisorderGroup_id DisorderGroup_Name  \\\n",
       "0            21394           Disease             36547           Disorder   \n",
       "1            21394           Disease             36547           Disorder   \n",
       "2            21394           Disease             36547           Disorder   \n",
       "3            21394           Disease             36547           Disorder   \n",
       "4            21394           Disease             36547           Disorder   \n",
       "\n",
       "   HPODisorderAssociation_id  ...       HPOId                      HPOTerm  \\\n",
       "0                     327485  ...  HP:0000256                 Macrocephaly   \n",
       "1                     327486  ...  HP:0001249      Intellectual disability   \n",
       "2                     327487  ...  HP:0001250                     Seizures   \n",
       "3                     327488  ...  HP:0001257                   Spasticity   \n",
       "4                     327489  ...  HP:0001274  Agenesis of corpus callosum   \n",
       "\n",
       "  HPOFrequency_id       HPOFrequency_Name DiagnosticCriteria_id  \\\n",
       "0           28412  Very frequent (99-80%)                   NaN   \n",
       "1           28412  Very frequent (99-80%)                   NaN   \n",
       "2           28412  Very frequent (99-80%)                   NaN   \n",
       "3           28412  Very frequent (99-80%)                   NaN   \n",
       "4           28412  Very frequent (99-80%)                   NaN   \n",
       "\n",
       "   DiagnosticCriteria_Name Source ValidationStatus Online  \\\n",
       "0                      NaN    NaN                y      y   \n",
       "1                      NaN    NaN                y      y   \n",
       "2                      NaN    NaN                y      y   \n",
       "3                      NaN    NaN                y      y   \n",
       "4                      NaN    NaN                y      y   \n",
       "\n",
       "          ValidationDate  \n",
       "0  2016-06-01 00:00:00.0  \n",
       "1  2016-06-01 00:00:00.0  \n",
       "2  2016-06-01 00:00:00.0  \n",
       "3  2016-06-01 00:00:00.0  \n",
       "4  2016-06-01 00:00:00.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking converted dataset\n",
    "df = pd.read_csv(os.path.join(onto_dir_path, 'en_product4.csv'))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6058d15c",
   "metadata": {},
   "source": [
    "### Dataset to triples, entities and relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "793735e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_assoc = {  # from csv frequency to frequency code\n",
    "    'Obligate (100%)': 'O',\n",
    "    'Very frequent (99-80%)': 'VF',\n",
    "    'Frequent (79-30%)': 'F',\n",
    "    'Occasional (29-5%)': 'OC',\n",
    "    'Very rare (<4-1%)': 'VR',\n",
    "    'Excluded (0%)': 'E'\n",
    "}\n",
    "\n",
    "freq_code_assoc = {  # from frequency code to output class\n",
    "    'O': 'obligate',\n",
    "    'VF': 'very_frequent',\n",
    "    'F': 'frequent',\n",
    "    'OC': 'occasional',\n",
    "    'VR': 'very_rare',\n",
    "    'E': 'excluded'\n",
    "}\n",
    "\n",
    "dc_association = {  # default: exclusion\n",
    "    'Diagnostic criterion': 'diagnostic_criterion',\n",
    "    'Pathognomonic sign': 'pathognomonic_sign',\n",
    "}\n",
    "\n",
    "\n",
    "def get_association_subclass(orpha, freq, hp):\n",
    "    \"\"\"Returns normalized association class\n",
    "\n",
    "    Args:\n",
    "        orpha (str):\n",
    "            The prefixed Orphanet code\n",
    "        freq (str):\n",
    "            The frequency text\n",
    "        hp (str):\n",
    "            The prefixed HPO ID\n",
    "    Returns:\n",
    "        (str):\n",
    "            The orphacode, hpo id and frequency association\n",
    "    \"\"\"\n",
    "    return orpha + '_' + hp + '_FREQ:' + freq_assoc.get(freq)\n",
    "\n",
    "\n",
    "def get_association_name(orpha, freq, hp):\n",
    "    \"\"\"Returns textual description of the association class\n",
    "\n",
    "    Args:\n",
    "        orpha (str):\n",
    "            The prefixed Orphanet code\n",
    "        freq (str):\n",
    "            The frequency text\n",
    "        hp (str):\n",
    "            The prefixed HPO ID\n",
    "    Returns:\n",
    "        (str):\n",
    "            The orphacode, hpo id and frequency association \n",
    "            textual_description_with_underscores\n",
    "    \"\"\"\n",
    "    return get_normalized_string(orpha_entities.get(orpha) + ' and ' +\n",
    "                                 hpo_entities.get(hp) + ' ' +\n",
    "                                 freq_code_assoc.get(freq_assoc.get(freq)) +\n",
    "                                 ' association')\n",
    "\n",
    "\n",
    "def get_normalized_string(s):\n",
    "    \"\"\"Transforms a string to lowercase and replaces all whitespace runs with an underscore\n",
    "\n",
    "    Args:\n",
    "        s (str):\n",
    "            String to normalize\n",
    "    Returns:\n",
    "        (str):\n",
    "            The normalized string\n",
    "    \"\"\"\n",
    "    return re.sub(r\"\\s+\", '_', s.lower())\n",
    "\n",
    "\n",
    "df_dataset = pd.read_csv(os.path.join(onto_dir_path, 'en_product4.csv'), \n",
    "                         dtype='object')\n",
    "df_dataset['OrphaCode'] = df_dataset['OrphaCode'].map(lambda x: 'ORPHA:' + x)\n",
    "\n",
    "# key is id, value is textual_description_with_underscores\n",
    "assoc_entities = {}\n",
    "dc_entities = {'diagnostic_criterion': 'diagnostic_criterion',\n",
    "               'pathognomonic_sign': 'pathognomonic_sign',\n",
    "               'exclusion': 'exclusion'}\n",
    "freq_assoc_entities = {'obligate': 'obligate', 'very_frequent': 'very_frequent',\n",
    "                       'frequent': 'frequent', 'occasional': 'occasional',\n",
    "                       'very_rare': 'very_rare', 'excluded': 'excluded'}\n",
    "hpo_entities = {}\n",
    "orpha_entities = {}\n",
    "\n",
    "has_object_triples = []  # association has_object HPOId\n",
    "has_subject_triples = []  # association has_subject OrphaCode\n",
    "has_frequency_triples = []  # association has_frequency FrequencyAssociation\n",
    "has_diagnostic_criterion_triples = []  # association has_DC_attribute DC\n",
    "\n",
    "\n",
    "# reading the dataset\n",
    "for orpha, freq, hp, dc, \\\n",
    "    orpha_name, hpo_name in zip(df_dataset['OrphaCode'],\n",
    "                                df_dataset['HPOFrequency_Name'],\n",
    "                                df_dataset['HPOId'],\n",
    "                                df_dataset['DiagnosticCriteria_Name'],\n",
    "                                df_dataset['Name'],\n",
    "                                df_dataset['HPOTerm']):\n",
    "    if hp not in hpo_entities:\n",
    "        hpo_entities[hp] = get_normalized_string(hpo_name)\n",
    "    if orpha not in orpha_entities:\n",
    "        orpha_entities[orpha] = get_normalized_string(orpha_name)\n",
    "\n",
    "    ac = get_association_subclass(orpha, freq, hp)\n",
    "    ac_name = get_association_name(orpha, freq, hp)\n",
    "    assoc_entities[ac] = ac_name\n",
    "\n",
    "    has_object_triples.append((ac, 'association_has_object', hp))\n",
    "    has_subject_triples.append((ac, 'association_has_subject', orpha))\n",
    "    has_frequency_triples.append((ac, 'has_frequency', freq_code_assoc.get(freq_assoc.get(freq))))\n",
    "    has_diagnostic_criterion_triples.append((ac, 'has_DC_attribute', dc_association.get(dc, 'exclusion')))\n",
    "\n",
    "\n",
    "# lists corresponding to each output file\n",
    "triples = []\n",
    "triples_names = []\n",
    "entities = []\n",
    "entities_names = []\n",
    "relations = []\n",
    "\n",
    "# subClassOf triples\n",
    "entities_and_parent_class = [\n",
    "    (assoc_entities, 'association'),\n",
    "    (dc_entities, 'diagnostic_criteria'), \n",
    "    (freq_assoc_entities, 'frequency_association'), \n",
    "    (hpo_entities, 'HPO_Id'), \n",
    "    (orpha_entities, 'OrphaCode')\n",
    "]\n",
    "for (entities_dict, parent_class) in entities_and_parent_class:\n",
    "    for k, v in entities_dict.items(): \n",
    "        triples.append((k, 'subClassOf', get_normalized_string(parent_class)))\n",
    "        triples_names.append((v, 'subClassOf', parent_class))\n",
    "\n",
    "# other properties triples\n",
    "triples_and_entities = [\n",
    "    (has_object_triples, hpo_entities), \n",
    "    (has_subject_triples, orpha_entities), \n",
    "    (has_frequency_triples, freq_assoc_entities), \n",
    "    (has_diagnostic_criterion_triples, dc_entities)\n",
    "]\n",
    "for (properties_triples, entities_dict) in triples_and_entities:\n",
    "    for (s, r, o) in properties_triples:\n",
    "        triples.append((s, r, o))\n",
    "        triples_names.append((assoc_entities.get(s), r, entities_dict.get(o)))\n",
    "\n",
    "# parent entities\n",
    "for i, (k, v) in enumerate(entities_and_parent_class):\n",
    "    parent = get_normalized_string(v)\n",
    "    entities.append((i, parent))\n",
    "    entities_names.append((i, parent))\n",
    "# entities\n",
    "parents_count = len(entities)\n",
    "for i, (k, v) in enumerate({**assoc_entities, **dc_entities, **freq_assoc_entities,\n",
    "                            **hpo_entities, **orpha_entities}.items()):\n",
    "    entities.append((i+parents_count, k))\n",
    "    entities_names.append((i+parents_count, v))\n",
    "\n",
    "# relations\n",
    "for i, r in enumerate(['subClassOf', 'association_has_object',\n",
    "                       'association_has_subject', 'has_frequency', \n",
    "                       'has_DC_attribute']):\n",
    "    relations.append((i, r))\n",
    "\n",
    "\n",
    "# writing to the different files\n",
    "lists_and_files = [\n",
    "    (triples, 'triples.txt'), \n",
    "    (triples_names, 'triples_names.txt'),\n",
    "    (entities, 'entities.dict'),\n",
    "    (entities_names, 'entities_names.dict'),\n",
    "    (relations, 'relations.dict')\n",
    "]\n",
    "for (l, n) in lists_and_files:\n",
    "    with open(os.path.join(onto_dir_path, n), 'w') as f:\n",
    "        for t in l:\n",
    "            f.write('\\t'.join(str(e) for e in t) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8253c004",
   "metadata": {},
   "source": [
    "### Merge ORDO, HP and HOOM ontologies using Protégé"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce210777",
   "metadata": {},
   "source": [
    "https://bioportal.bioontology.org/ontologies/ORDO?p=summary \n",
    "    \n",
    "https://bioportal.bioontology.org/ontologies/HP?p=summary \n",
    "\n",
    "https://bioportal.bioontology.org/ontologies/HOOM?p=summary: 'HOOM is a module that qualifies the annotation between a clinical entity and phenotypic abnormalities according to a frequency and by integrating the notion of diagnostic criterion.'\n",
    "\n",
    "Using Protégé, merge HP (in OBO format, very important) into HOOM (OWL) and then ORDO (OWL) into the result of the merge to obtain a merge of the 3 ontologies, then export in OWL/XML format, and export in turtle format too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e11616-2b13-4504-8b6e-7eaf2c0587ec",
   "metadata": {},
   "source": [
    "### Seen and unseen RDs from the ontology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e116e381-0d45-474b-95c7-de21b7ef8522",
   "metadata": {},
   "source": [
    "#### Loading the ontology graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "id": "cc68bf84-f316-42fe-af3c-0a84c4422527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len g: 4162248\n"
     ]
    }
   ],
   "source": [
    "g = rdflib.Graph()\n",
    "g.parse('../persistent/data/ontology/hpObo_hoom_ordo.ttl')\n",
    "\n",
    "print(f'len g: {len(g)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be932593-cd14-4a8f-b1c2-d1e302f7fe9b",
   "metadata": {},
   "source": [
    "#### Get all RDs and their corresponding groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "8317546a-a855-4d1a-a3dc-688301a708ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# part of <http://www.orpha.net/ORDO/Orphanet_C021>\n",
    "\n",
    "groups_query = \"\"\"\n",
    "SELECT ?rd_g ?rd\n",
    "WHERE {\n",
    "    ?rd_uri rdf:type owl:Class;\n",
    "            <http://www.orpha.net/ORDO/Orphanet_C021> ?rd_g .\n",
    "    BIND (STR(?rd_uri) AS ?rd) .\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "groups_qres = g.query(groups_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc77ed8-1885-4e8f-bd42-f2b2a4c59f31",
   "metadata": {},
   "source": [
    "#### Get RDs seen in Association classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "3f695049-f2e6-45a0-9ebb-345bbfa12d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "rds_query = \"\"\"\n",
    "SELECT DISTINCT ?rd\n",
    "{\n",
    "    ?subject rdfs:subClassOf :Association .\n",
    "    BIND(REPLACE(STR(?subject), \".*Orpha:([0-9]+).*\", \"$1\") AS ?sub) .\n",
    "    BIND(CONCAT(\"http://www.orpha.net/ORDO/Orphanet_\", ?sub) AS ?rd) .\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "rds_qres = g.query(rds_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753097fa-880d-4f59-b4bd-ca6ec9db26f3",
   "metadata": {},
   "source": [
    "#### Get RDs from a specific subgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13f1ba64-a681-4dde-9f2e-06475fcaac64",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'g' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_365313/1860171808.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \"\"\"\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mselected_group_qres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_group_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselected_group_qres\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{row.label}: {row.rd}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'g' is not defined"
     ]
    }
   ],
   "source": [
    "# http://www.orpha.net/ORDO/Orphanet_182111: Respiratory malformation\n",
    "\n",
    "selected_group_query = \"\"\"\n",
    "SELECT DISTINCT ?rd ?label\n",
    "WHERE {\n",
    "    ?subject rdfs:subClassOf :Association .\n",
    "    BIND(REPLACE(STR(?subject), \".*Orpha:([0-9]+).*\", \"$1\") AS ?sub) .\n",
    "    BIND(CONCAT(\"http://www.orpha.net/ORDO/Orphanet_\", ?sub) AS ?rd) .\n",
    "    BIND (URI(?rd) AS ?rd_uri) .\n",
    "\n",
    "    ?rd_uri <http://www.orpha.net/ORDO/Orphanet_C021> \"http://www.orpha.net/ORDO/Orphanet_182111\" ;\n",
    "            rdfs:label ?label .\n",
    "    BIND (STR(?rd_uri) AS ?rd) .\n",
    "    FILTER REGEX(?label, \"[^ORPHA:0-9]\") .\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "selected_group_qres = g.query(selected_group_query)\n",
    "for row in selected_group_qres:\n",
    "    print(f'{row.label}: {row.rd}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "id": "0ae71154-f128-47df-bd47-872a14a5203b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta-mannosidosis: http://www.orpha.net/ORDO/Orphanet_118\n",
      "Adrenomyeloneuropathy: http://www.orpha.net/ORDO/Orphanet_139399\n",
      "Abetalipoproteinemia: http://www.orpha.net/ORDO/Orphanet_14\n",
      "Adult polyglucosan body disease: http://www.orpha.net/ORDO/Orphanet_206583\n",
      "Choreoacanthocytosis: http://www.orpha.net/ORDO/Orphanet_2388\n",
      "De Barsy syndrome: http://www.orpha.net/ORDO/Orphanet_2962\n",
      "Tangier disease: http://www.orpha.net/ORDO/Orphanet_31150\n",
      "Fabry disease: http://www.orpha.net/ORDO/Orphanet_324\n",
      "Homocystinuria due to methylene tetrahydrofolate reductase deficiency: http://www.orpha.net/ORDO/Orphanet_395\n",
      "Gyrate atrophy of choroid and retina: http://www.orpha.net/ORDO/Orphanet_414\n",
      "Cataract-growth hormone deficiency-sensory neuropathy-sensorineural hearing loss-skeletal dysplasia syndrome: http://www.orpha.net/ORDO/Orphanet_436174\n",
      "Non-progressive predominantly posterior cavitating leukoencephalopathy with peripheral neuropathy: http://www.orpha.net/ORDO/Orphanet_436271\n",
      "Ocular anomalies-axonal neuropathy-developmental delay syndrome: http://www.orpha.net/ORDO/Orphanet_496790\n",
      "Metachromatic leukodystrophy: http://www.orpha.net/ORDO/Orphanet_512\n",
      "Optic atrophy-ataxia-peripheral neuropathy-global developmental delay syndrome: http://www.orpha.net/ORDO/Orphanet_543470\n",
      "Long chain 3-hydroxyacyl-CoA dehydrogenase deficiency: http://www.orpha.net/ORDO/Orphanet_5\n",
      "Mitochondrial trifunctional protein deficiency: http://www.orpha.net/ORDO/Orphanet_746\n",
      "Niemann-Pick disease type B: http://www.orpha.net/ORDO/Orphanet_77293\n",
      "Refsum disease: http://www.orpha.net/ORDO/Orphanet_773\n",
      "Congenital bile acid synthesis defect type 4: http://www.orpha.net/ORDO/Orphanet_79095\n",
      "Biotinidase deficiency: http://www.orpha.net/ORDO/Orphanet_79241\n",
      "Methylmalonic acidemia with homocystinuria, type cblC: http://www.orpha.net/ORDO/Orphanet_79282\n",
      "Sandhoff disease: http://www.orpha.net/ORDO/Orphanet_796\n",
      "Tay-Sachs disease: http://www.orpha.net/ORDO/Orphanet_845\n",
      "Tyrosinemia type 1: http://www.orpha.net/ORDO/Orphanet_882\n",
      "Wilson disease: http://www.orpha.net/ORDO/Orphanet_905\n",
      "Cerebrotendinous xanthomatosis: http://www.orpha.net/ORDO/Orphanet_909\n",
      "Ataxia with vitamin E deficiency: http://www.orpha.net/ORDO/Orphanet_96\n"
     ]
    }
   ],
   "source": [
    "# http://www.orpha.net/ORDO/Orphanet_207018: Rare hereditary metabolic disease with peripheral neuropathy (leukemia-related)\n",
    "\n",
    "selected_group_query = \"\"\"\n",
    "SELECT DISTINCT ?rd ?label\n",
    "WHERE {\n",
    "    ?subject rdfs:subClassOf :Association .\n",
    "    BIND(REPLACE(STR(?subject), \".*Orpha:([0-9]+).*\", \"$1\") AS ?sub) .\n",
    "    BIND(CONCAT(\"http://www.orpha.net/ORDO/Orphanet_\", ?sub) AS ?rd) .\n",
    "    BIND (URI(?rd) AS ?rd_uri) .\n",
    "\n",
    "    ?rd_uri <http://www.orpha.net/ORDO/Orphanet_C021> \"http://www.orpha.net/ORDO/Orphanet_207018\" ;\n",
    "            rdfs:label ?label .\n",
    "    BIND (STR(?rd_uri) AS ?rd) .\n",
    "    FILTER REGEX(?label, \"[^ORPHA:0-9]\") .\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "selected_group_qres = g.query(selected_group_query)\n",
    "for row in selected_group_qres:\n",
    "    print(f'{row.label}: {row.rd}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5febcb6c-f2b4-493f-a6b4-a5432374cd48",
   "metadata": {},
   "source": [
    "#### Get RDs containing a certain word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "id": "ee8dc7fb-e63d-4098-b21c-55730d732c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acute erythroid leukemia: http://www.orpha.net/ORDO/Orphanet_318\n",
      "Deafness-lymphedema-leukemia syndrome: http://www.orpha.net/ORDO/Orphanet_3226\n",
      "Acute monoblastic/monocytic leukemia: http://www.orpha.net/ORDO/Orphanet_514\n",
      "Acute myelomonocytic leukemia: http://www.orpha.net/ORDO/Orphanet_517\n",
      "Acute promyelocytic leukemia: http://www.orpha.net/ORDO/Orphanet_520\n",
      "Chronic myeloid leukemia: http://www.orpha.net/ORDO/Orphanet_521\n"
     ]
    }
   ],
   "source": [
    "selected_group_query = \"\"\"\n",
    "SELECT DISTINCT ?rd ?label\n",
    "WHERE {\n",
    "    ?subject rdfs:subClassOf :Association .\n",
    "    BIND(REPLACE(STR(?subject), \".*Orpha:([0-9]+).*\", \"$1\") AS ?sub) .\n",
    "    BIND(CONCAT(\"http://www.orpha.net/ORDO/Orphanet_\", ?sub) AS ?rd) .\n",
    "    BIND (URI(?rd) AS ?rd_uri) .\n",
    "\n",
    "    ?rd_uri rdfs:label ?label .\n",
    "    FILTER CONTAINS(?label, \"leukemia\") .\n",
    "    FILTER REGEX(?label, \"[^ORPHA:0-9]\") .\n",
    "    BIND (STR(?rd_uri) AS ?rd) .\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "selected_group_qres = g.query(selected_group_query)\n",
    "for row in selected_group_qres:\n",
    "    print(f'{row.label}: {row.rd}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7ede9a-5e71-4b44-a03b-931a495c8ae1",
   "metadata": {},
   "source": [
    "#### Building a dictionary of RD groups and their RDs seen in Association classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "id": "941eb583-c4ba-43dd-9e6e-bbcb128c0333",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique, seen in Association classes RDs: 4345 4345\n",
      "Number of RD groups: 1692\n",
      "Total number of RDs in groups: 10484\n",
      "Average: 6.196217494089835\n"
     ]
    }
   ],
   "source": [
    "rd_list = []\n",
    "rd_groups_dict = {}\n",
    "\n",
    "# set of all RDs\n",
    "for row in rds_qres:\n",
    "    rd_list.append(str(row.rd))\n",
    "rd_set = set(rd_list)\n",
    "\n",
    "# RD groups dictionary\n",
    "for row in groups_qres:\n",
    "    rd = str(row.rd)\n",
    "    rd_g = str(row.rd_g)\n",
    "    if rd in rd_set:\n",
    "        if rd_g not in rd_groups_dict:\n",
    "            rd_groups_dict[rd_g] = []\n",
    "        if rd not in rd_groups_dict[rd_g]:\n",
    "            rd_groups_dict[rd_g].append(rd)\n",
    "\n",
    "print(f'Number of unique, seen in Association classes RDs: {len(rd_list)} {len(rd_set)}\\nNumber of RD groups: {len(rd_groups_dict)}')\n",
    "\n",
    "len_sum = sum(len(dct) for dct in rd_groups_dict.values())\n",
    "\n",
    "print(f'Total number of RDs in groups: {len_sum}\\nAverage: {len_sum/len(rd_groups_dict)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84667fa1-3c29-4039-acbd-d1a032b2cac8",
   "metadata": {},
   "source": [
    "#### Building sets of seen and unseen RDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "id": "bd439517-110d-41fb-bd16-39813ec07438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of seen RDs: 12 (0.28%)\n",
      "Number of unseen RDs: 5 (0.12%)\n"
     ]
    }
   ],
   "source": [
    "# seen and unseen RDs sets\n",
    "seen_rds_set = set()\n",
    "unseen_rds_set = set()\n",
    "\n",
    "\n",
    "def populating_rd_sets(method='no_groups', shuffle=False, unseen_pct=0.2, selected_seen_rds=[], selected_unseen_rds=[]):\n",
    "    \"\"\"Builds sets of seen and unseen RDs for synthetic data generation.\n",
    "       The populated sets are global variables outside this function.\n",
    "\n",
    "    Args:\n",
    "        method (str):\n",
    "            Method to choose to build the sets, one of 'no_groups', 'parts_groups', 'whole_groups', 'selected_rds':\n",
    "            'no_groups' adds a guaranteed percentage (unseen_pct) of all RDs to unseen RDs, regardless of their groups.\n",
    "            'parts_groups' adds a percentage (unseen_pct) of RDs inside groups to unseen RDs, \n",
    "                eventually stopping if unseen_rds_set_set exceeds the unseen_pct of the RDs total.\n",
    "            'whole_groups' adds all RDs of each group to unseen RDs until unseen_rds_set_set exceeds the unseen_pct of the RDs total.\n",
    "            'selected_rds' only adds the selected rds to unseen RDs.\n",
    "            Defaults to 'no_groups'.\n",
    "        shuffle (bool)\n",
    "            Whether to shuffle the order of the rd groups dictionary. Defaults to False.\n",
    "        rds_pct (float):\n",
    "            The wished ratio of unseen RDs. Defaults to 0.2.\n",
    "        selected_seen_rds (list-like):\n",
    "            The list-like object of the IRIs of seen RDs, only used if the method is 'selected_rds'.\n",
    "            If empty while selected_unseen_rds is not empty, all remaining RDs will be \"seen\".\n",
    "        selected_unseen_rds (list-like):\n",
    "            The list-like object of the IRIs of unseen RDs, only used if the method is 'selected_rds'.\n",
    "    \"\"\"\n",
    "\n",
    "    methods = {'no_groups', 'parts_groups', 'whole_groups', 'selected_rds'}\n",
    "    if method not in methods:\n",
    "        raise Exception(f'Unknown method \"{method}\", should be one of {methods}')\n",
    "\n",
    "    total_rds = len(rd_set)\n",
    "\n",
    "    # building the sets\n",
    "    if method == 'no_groups':\n",
    "        indexes = [i for i in range(len(rd_list))]\n",
    "        if shuffle:\n",
    "            random.shuffle(indexes)\n",
    "        for ind in indexes:  # iterating on all RDs\n",
    "            rd = rd_list[ind]\n",
    "            if (rd not in seen_rds_set) and (rd not in unseen_rds_set):\n",
    "                if len(unseen_rds_set) > total_rds*unseen_pct:\n",
    "                    seen_rds_set.add(rd)\n",
    "                else:\n",
    "                    unseen_rds_set.add(rd)\n",
    "\n",
    "    elif method == 'parts_groups':\n",
    "        keys = list(rd_groups_dict.keys())\n",
    "        if shuffle:\n",
    "            random.shuffle(keys)\n",
    "        for rd_group in keys:  # iterating on groups\n",
    "            rds = rd_groups_dict[rd_group]\n",
    "            # counting unseen RDs to stop at rds*pct threshold\n",
    "            unseen_cnt = 0\n",
    "            if len(unseen_rds_set) > total_rds*unseen_pct:\n",
    "                # no more RDs added to unseen RDs if unseen_rds_set is over the threshold\n",
    "                unseen_cnt = len(unseen_rds_set)\n",
    "            for rd in rds:\n",
    "                if rd in unseen_rds_set:\n",
    "                    unseen_cnt += 1\n",
    "                else:\n",
    "                    if rd not in seen_rds_set:\n",
    "                        if unseen_cnt > len(rds)*unseen_pct:\n",
    "                            seen_rds_set.add(rd)\n",
    "                        else:\n",
    "                            unseen_rds_set.add(rd)\n",
    "                            unseen_cnt += 1\n",
    "\n",
    "    elif method == 'whole_groups':  # pct of unseen RDs not guaranteed\n",
    "        keys = list(rd_groups_dict.keys())\n",
    "        if shuffle:\n",
    "            random.shuffle(keys)\n",
    "        for rd_group in keys:  # iterating on groups\n",
    "            rds = rd_groups_dict[rd_group]\n",
    "            if len(unseen_rds_set) > total_rds*unseen_pct:\n",
    "                for rd in rds:\n",
    "                    if (rd not in seen_rds_set) and (rd not in unseen_rds_set):\n",
    "                        seen_rds_set.add(rd)\n",
    "            else:\n",
    "                for rd in rds:\n",
    "                    if rd not in unseen_rds_set:\n",
    "                        unseen_rds_set.add(rd)\n",
    "\n",
    "    elif method == 'selected_rds':\n",
    "        for rd in selected_unseen_rds:  # iterating on chosen groups\n",
    "            if rd in rd_set:\n",
    "                if rd not in unseen_rds_set:\n",
    "                    unseen_rds_set.add(rd)\n",
    "            else:\n",
    "                raise Exception(f'Unknown RD {rd}')\n",
    "        # adding the seen_rds_set\n",
    "        if len(selected_seen_rds) > 0:\n",
    "            for rd in selected_seen_rds:  # iterating on chosen groups\n",
    "                if rd in rd_set:\n",
    "                    if (rd not in seen_rds_set) and (rd not in unseen_rds_set):\n",
    "                        seen_rds_set.add(rd)\n",
    "                else:\n",
    "                    raise Exception(f'Unknown RD {rd}')\n",
    "        else:  # adding all remaining RDs\n",
    "            for rds in rd_groups_dict.values():\n",
    "                for rd in rds:\n",
    "                    if (rd not in seen_rds_set) and (rd not in unseen_rds_set):\n",
    "                        seen_rds_set.add(rd)\n",
    "\n",
    "    if method not in ['no_groups', 'selected_rds']:\n",
    "        # TODO: allow some of these to be added to unseen_rds_set?\n",
    "        # adding the RDs that aren't part of a RD group, in the seen set\n",
    "        for rd in rd_list:\n",
    "            if (rd not in seen_rds_set) and (rd not in unseen_rds_set):\n",
    "                seen_rds_set.add(rd)\n",
    "\n",
    "    print(f'Number of seen RDs: {len(seen_rds_set)} ({(len(seen_rds_set)/len(rd_set)*100):.2f}%)\\nNumber of unseen RDs: {len(unseen_rds_set)} ({(len(unseen_rds_set)/len(rd_set)*100):.2f}%)')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Selected RDs from 2 disorder subgroups (5 seen + 2/3 unseen for each group)\n",
    "# Leukodystrophy (brain) and Respiratory diseases\n",
    "seen = [\n",
    "    \"http://www.orpha.net/ORDO/Orphanet_1120\",  # Lung agenesis-heart defect-thumb anomalies syndrome\n",
    "    \"http://www.orpha.net/ORDO/Orphanet_2414\",  # Congenital pulmonary lymphangiectasia\n",
    "    \"http://www.orpha.net/ORDO/Orphanet_2444\",  # Congenital pulmonary airway malformation\n",
    "    \"http://www.orpha.net/ORDO/Orphanet_2470\",  # Matthew-Wood syndrome\n",
    "    \"http://www.orpha.net/ORDO/Orphanet_2257\",  # Primary pulmonary hypoplasia\n",
    "    \"http://www.orpha.net/ORDO/Orphanet_135\",  # CACH syndrome\n",
    "    \"http://www.orpha.net/ORDO/Orphanet_141\",  # Canavan disease\n",
    "    \"http://www.orpha.net/ORDO/Orphanet_512\",  # Metachromatic leukodystrophy\n",
    "    \"http://www.orpha.net/ORDO/Orphanet_43\",  # X-linked adrenoleukodystrophy\n",
    "    \"http://www.orpha.net/ORDO/Orphanet_139444\"  # Leukoencephalopathy with bilateral anterior temporal lobe cysts\n",
    "]\n",
    "unseen = [\n",
    "    \"http://www.orpha.net/ORDO/Orphanet_3346\",  # Tracheal agenesis\n",
    "    \"http://www.orpha.net/ORDO/Orphanet_185\",  # Scimitar syndrome\n",
    "    \"http://www.orpha.net/ORDO/Orphanet_774\",  # Hereditary hemorrhagic telangiectasia\n",
    "    \"http://www.orpha.net/ORDO/Orphanet_466934\",  # VPS11-related autosomal recessive hypomyelinating leukodystrophy\n",
    "    \"http://www.orpha.net/ORDO/Orphanet_909\"  # Cerebrotendinous xanthomatosis\n",
    "]\"\"\"\n",
    "\n",
    "# Leukemia RDs\n",
    "seen = [\n",
    "    \"http://www.orpha.net/ORDO/Orphanet_231401\",  # Alpha-thalassemia-myelodysplastic syndrome\n",
    "    \"http://www.orpha.net/ORDO/Orphanet_29073\",  # Multiple myeloma\n",
    "    \"http://www.orpha.net/ORDO/Orphanet_729\",  # Polycythemia vera\n",
    "    \"http://www.orpha.net/ORDO/Orphanet_3226\",  # Deafness-lymphedema-leukemia syndrome\n",
    "    \"http://www.orpha.net/ORDO/Orphanet_86843\",  # Acute panmyelosis with myelofibrosis\n",
    "    \"http://www.orpha.net/ORDO/Orphanet_98827\",  # Unclassified myelodysplastic syndrome\n",
    "    \"http://www.orpha.net/ORDO/Orphanet_514\",  # Acute monoblastic/monocytic leukemia\n",
    "    \"http://www.orpha.net/ORDO/Orphanet_517\",  # Acute myelomonocytic leukemia\n",
    "    \"http://www.orpha.net/ORDO/Orphanet_318\",  # Acute erythroid leukemia\n",
    "    \"http://www.orpha.net/ORDO/Orphanet_824\",  # Primary myelofibrosis\n",
    "    \"http://www.orpha.net/ORDO/Orphanet_520\",  # Acute promyelocytic leukemia\n",
    "    \"http://www.orpha.net/ORDO/Orphanet_139399\"  # Adrenomyeloneuropathy\n",
    "]\n",
    "unseen = [\n",
    "    \"http://www.orpha.net/ORDO/Orphanet_3318\",  # Essential thrombocythemia\n",
    "    \"http://www.orpha.net/ORDO/Orphanet_521\",  # Chronic myeloid leukemia\n",
    "    \"http://www.orpha.net/ORDO/Orphanet_512\"  # Metachromatic leukodystrophy\n",
    "]\n",
    "\n",
    "populating_rd_sets(method='selected_rds', selected_seen_rds=seen, selected_unseen_rds=unseen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4d9077-ef7a-45c5-b5f3-c6361b8a9000",
   "metadata": {},
   "source": [
    "### Synthetic data generation from the ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06240382-5159-4e7d-8e3e-e35997620c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_dict = {  # frequency ids + associated probability\n",
    "    28405: 1,  # Obligate (100%)\n",
    "    28412: 0.895,  # Very frequent (99-80%)\n",
    "    28419: 0.545,  # Frequent (79-30%)\n",
    "    28426: 0.17,  # Occasional (29-5%)\n",
    "    28433: 0.025,  # Very rare (<4-1%)\n",
    "    28440: 0  # Excluded (0%)\n",
    "}\n",
    "\n",
    "\n",
    "def get_orpha_iri(orphacode):\n",
    "    \"\"\"Gets the IRI of an OrphaCode.\n",
    "\n",
    "    Args:\n",
    "        orphacode (int):\n",
    "            The unique code of the rare disease in the ORDO ontology.\n",
    "    Returns:\n",
    "        (str):\n",
    "            The Orphanet IRI.\n",
    "    \"\"\"\n",
    "    return f'http://www.orpha.net/ORDO/Orphanet_{orphacode}'\n",
    "\n",
    "\n",
    "def gen_syn_data(patients_per_rd=10, use_ontology_rds=True, gen_small_file=False, print_every=0, del_col_th=0, sort=True):\n",
    "    \"\"\"Generates synthetic seen and unseen data from the ontology into files.\n",
    "\n",
    "    Args:\n",
    "        patients_per_rd (int):\n",
    "            Number of generated patients per RD. Defaults to 10.\n",
    "        use_ontology_rds (bool):\n",
    "            Whether to use the seen and unseen RD sets obtained from ontology queries.\n",
    "            Defaults to True.\n",
    "        gen_small_file (bool):\n",
    "            Whether to generate very small files to debug the model or the full ones.\n",
    "            Defaults to False.\n",
    "        print_every (int):\n",
    "            Each number of patients to print progress during data generation (0 = no print).\n",
    "            Defaults to 0.\n",
    "        del_col_th (int):\n",
    "            How few values for a column to be deleted (not inclusive), 0 for no deletion. Defaults to 0.\n",
    "        sort (str):\n",
    "            Whether to alphabetically sort RDs in output files. Defaults to True.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(os.path.join(onto_dir_path, 'en_product4.csv'))\n",
    "\n",
    "    # Deleting RDs not seen in the ontology.\n",
    "    df['Orpha_URI'] = 'http://www.orpha.net/ORDO/Orphanet_' + df['OrphaCode'].astype(str)\n",
    "    df = df.loc[df['Orpha_URI'].isin(seen_rds_set.union(unseen_rds_set))]\n",
    "\n",
    "    if gen_small_file:\n",
    "        # only the first 10 rows\n",
    "        grouped = df.groupby('Name', sort=False)\n",
    "        df = pd.concat([group for name, group in grouped][:10])\n",
    "    grouped = df.groupby('Name', sort=False)\n",
    "\n",
    "    rd_count = grouped.ngroups\n",
    "    unique_hps = df.HPOTerm.unique()\n",
    "    total_hp_count = len(unique_hps)\n",
    "\n",
    "    print(f'{rd_count} unique rare diseases, {total_hp_count} unique phenotypes')\n",
    "\n",
    "    # Dictionary of phenotypes and their position in the data row\n",
    "    phenotypes = unique_hps.tolist()\n",
    "    phenotypes_dict = {hp: i for i, hp in enumerate(phenotypes)}\n",
    "\n",
    "    header = [['rare_disease'] + phenotypes]\n",
    "    seen_patients_data = []\n",
    "    unseen_patients_data = []\n",
    "\n",
    "    distribution_check = {  # value: count of patients with hp + maximum count of patients that could've had the hp\n",
    "        28405: [0, 0],  # Obligate (100%)\n",
    "        28412: [0, 0],  # Very frequent (99-80%)\n",
    "        28419: [0, 0],  # Frequent (79-30%)\n",
    "        28426: [0, 0],  # Occasional (29-5%)\n",
    "        28433: [0, 0],  # Very rare (<4-1%)\n",
    "        28440: [0, 0]  # Excluded (0%)\n",
    "    }\n",
    "\n",
    "    patients_count = 0\n",
    "\n",
    "    if not use_ontology_rds:\n",
    "        # Only seen RDs are generated if the sets built from the ontology aren't used\n",
    "        vectorize = np.vectorize(get_orpha_iri)\n",
    "        seen_rds = vectorize(df.OrphaCode.unique())\n",
    "        unseen_rds = []\n",
    "\n",
    "    for group_nb, (name, group) in enumerate(grouped):  # for each RD\n",
    "        hp_count = len(group)\n",
    "        # generate patients_per_rd patients for each RD\n",
    "        for patient in range(patients_count, patients_count+patients_per_rd):\n",
    "            temp_hp = []\n",
    "            proba_results = np.random.rand(hp_count)  # generating random floats for probabilities\n",
    "            rd_n = ''\n",
    "            rd_iri = ''\n",
    "            for i, (orphacode, rd_name, hp_name, frequency_id) in enumerate(zip(\n",
    "                                                            group['OrphaCode'],\n",
    "                                                            group['Name'],\n",
    "                                                            group['HPOTerm'],\n",
    "                                                            group['HPOFrequency_id'])):\n",
    "                distribution_check.get(frequency_id)[1] += 1\n",
    "                if rd_n == '':\n",
    "                    rd_n = rd_name\n",
    "                    rd_iri = get_orpha_iri(orphacode)\n",
    "                if (proba_results[i] >= 1 - frequency_dict[frequency_id]):  # comparing generated float and proba\n",
    "                    temp_hp.append(hp_name)\n",
    "                    distribution_check.get(frequency_id)[0] += 1\n",
    "\n",
    "            if len(temp_hp) > 0:\n",
    "                row = np.zeros((total_hp_count,), dtype=int)\n",
    "                for hp in temp_hp:\n",
    "                    row[phenotypes_dict.get(hp)] = 1\n",
    "                if use_ontology_rds:\n",
    "                    if rd_iri in seen_rds_set:\n",
    "                        seen_patients_data.append(np.concatenate([[rd_n], row]))\n",
    "                    elif rd_iri in unseen_rds_set:\n",
    "                        unseen_patients_data.append(np.concatenate([[rd_n], row]))\n",
    "                    else:\n",
    "                        print(f'Unknown RD {rd_n}')\n",
    "                        break  # skipping unknown RD\n",
    "                else:\n",
    "                    if rd_iri in seen_rds:\n",
    "                        seen_patients_data.append(np.concatenate([[rd_n], row]))\n",
    "                    elif rd_iri in unseen_rds:\n",
    "                        unseen_patients_data.append(np.concatenate([[rd_n], row]))\n",
    "                    else:\n",
    "                        print(f'Unknown RD {rd_n}')\n",
    "                        break  # skipping unknown RD\n",
    "\n",
    "                if print_every > 0:\n",
    "                    if patients_count % print_every == 0:\n",
    "                        print(f'{patients_count}/{patients_per_rd*rd_count} patients generated')\n",
    "\n",
    "                patients_count += 1\n",
    "\n",
    "    # index at which the unseen patients data starts\n",
    "    unseen_index = len(seen_patients_data) + 1\n",
    "    full_data = np.array(header + seen_patients_data + unseen_patients_data)\n",
    "\n",
    "    if del_col_th > 0:\n",
    "        # remove columns with < del_col_th patients that have this phenotype\n",
    "        col_sum = np.sum(full_data[1:, 1:].astype(int), axis=0)\n",
    "        mask = [True] + [s >= del_col_th for s in col_sum]\n",
    "        full_data = np.transpose(np.transpose(full_data)[mask])\n",
    "    seen_patients_data = full_data[:unseen_index]\n",
    "    headers = seen_patients_data[0]\n",
    "\n",
    "    # Build the final datasets\n",
    "    seen_patients_data = pd.DataFrame(seen_patients_data[1:], columns=headers)\n",
    "    seen_patients_data = seen_patients_data.loc[:, (seen_patients_data != '0').any(axis=0)]\n",
    "    if sort:\n",
    "        seen_patients_data = seen_patients_data.sort_values(by=['rare_disease'])\n",
    "\n",
    "    unseen_patients_data = full_data[unseen_index:]\n",
    "    unseen_unique = np.unique(unseen_patients_data[:, :1])\n",
    "    unseen_patients_data = pd.DataFrame(unseen_patients_data, columns=headers)\n",
    "    unseen_patients_data = unseen_patients_data[seen_patients_data.columns]\n",
    "    if sort:\n",
    "        unseen_patients_data = unseen_patients_data.sort_values(by=['rare_disease'])\n",
    "\n",
    "    print(f'{len(seen_patients_data)} seen patients generated ({len(seen_patients_data.columns)} columns), {len(unseen_patients_data)} unseen patients generated ({len(unseen_patients_data.columns)} columns), writing to files')\n",
    "\n",
    "    # writing the 2 data files and the list of unseen_rds\n",
    "    seen_patients_data.to_csv(\n",
    "        os.path.join(syn_data_dir_path, ('small_' if gen_small_file else '') + 'syn_patients_data_seen.csv'),\n",
    "        encoding='utf-8', index=False, header=True\n",
    "    )\n",
    "    if len(unseen_patients_data) > 0:\n",
    "        unseen_patients_data.to_csv(\n",
    "            os.path.join(syn_data_dir_path, ('small_' if gen_small_file else '') + 'syn_patients_data_unseen.csv'),\n",
    "            encoding='utf-8', index=False, header=True\n",
    "        )\n",
    "        with open(os.path.join(syn_data_dir_path, ('small_' if gen_small_file else '') + 'unseen_rds.txt'), 'w') as unseen_file:\n",
    "            for rd in unseen_unique:\n",
    "                unseen_file.write(f'{rd}\\n')\n",
    "\n",
    "    print(f'Total RDs: {rd_count}, {rd_count-len(unseen_unique)} seen RDs, {len(unseen_unique)} unseen RDs')\n",
    "\n",
    "    return distribution_check\n",
    "\n",
    "\n",
    "distributions = gen_syn_data(patients_per_rd=50, gen_small_file=False, print_every=1000, del_col_th=45, sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7cb223-15a1-49eb-be1b-16a408104071",
   "metadata": {},
   "source": [
    "#### Plotting the obtained HP frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "2efc1076-f81d-4e2d-bd27-a749b25ff15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(28405, [20, 20]), (28412, [282, 330]), (28419, [669, 1220]), (28426, [152, 900]), (28433, [3, 120]), (28440, [0, 10])]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_distributions(frequency_obs, frequency_th):\n",
    "    \"\"\"Visually compare theoretical and observed HP frequency.\n",
    "\n",
    "    Args:\n",
    "        frequency_obs (list-like):\n",
    "            List of observed frequency values.\n",
    "        frequency_th (list-like):\n",
    "            List of theoretical frequency values.\n",
    "    \"\"\"\n",
    "    index = np.arange(len(frequency_obs))\n",
    "    bar_width = 0.35\n",
    "\n",
    "    fix, ax = plt.subplots()\n",
    "\n",
    "    ax.bar(index, frequency_obs, bar_width, label='Observed')\n",
    "    ax.bar(index+bar_width, frequency_th, bar_width, label='Theoretical')\n",
    "\n",
    "    ax.set_xlabel('Frequency category')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title('Observed frequency VS theoretical frequency')\n",
    "    ax.set_xticks(index + bar_width / 2)\n",
    "    ax.set_xticklabels(['100%', '99-80%', '79-30%', '29-5%', '<4-1%', '0%'])\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "sorted_items = sorted(distributions.items(), key=lambda x: x[0])\n",
    "print(sorted_items)\n",
    "# Calculating the observed frequencies\n",
    "frequency_obs = [t[1][0]/t[1][1] if t[1][0] != 0 else t[1][0] for t in sorted_items]\n",
    "sorted_freq = sorted(frequency_dict.items(), key=lambda x: x[0])\n",
    "frequency_th = [x[1] for x in sorted_freq]\n",
    "\n",
    "show_distributions(frequency_obs, frequency_th)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdfde1c",
   "metadata": {},
   "source": [
    "### ORDO and HPO URIs dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "a01ce23c-0768-4088-bafe-c6b58d96bfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized names to ORDO URIs and HPO URIs\n",
    "\n",
    "df = pd.read_csv(os.path.join(onto_dir_path, 'en_product4.csv'))\n",
    "\n",
    "df_hp = df[['HPOTerm', 'HPOId']].drop_duplicates()\n",
    "df_rd = df[['Name', 'OrphaCode']].drop_duplicates()\n",
    "df_hp['HPOId'] = 'http://purl.obolibrary.org/obo/' + df_hp['HPOId'].str.replace(':', '_')\n",
    "df_rd['OrphaCode'] = 'http://www.orpha.net/ORDO/Orphanet_' + df_rd['OrphaCode'].astype(str)\n",
    "\n",
    "df_hp.to_csv(os.path.join(onto_dir_path, 'HPO.dict'), sep=';', encoding='utf-8', index=False, header=False)\n",
    "df_rd.to_csv(os.path.join(onto_dir_path, 'ORDO.dict'), sep=';', encoding='utf-8', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924d4ece",
   "metadata": {},
   "source": [
    "### Embedding the ontology with Owl2Vec*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1d6bf219-f3cf-4cb2-9d9b-38de47d1606f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence tokenizer\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed47500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = '../persistent/data/ontology/embeddings/hpObo_hoom_ordo'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "\"\"\"\n",
    "Parameters:\n",
    "    ontology_file\n",
    "    config_file\n",
    "    uri_doc\n",
    "    lit_doc\n",
    "    mix_doc\n",
    "    -> modify the cfg file for more params (cache dir, epochs, etc.)\n",
    "\"\"\"\n",
    "gensim_model = owl2vec_star.extract_owl2vec_model(None, \"./embedding.cfg\", True, True, True)\n",
    "\n",
    "# Gensim format\n",
    "gensim_model.save(os.path.join(output_folder, 'ontology.embeddings'))\n",
    "\n",
    "# Text format (not required)\n",
    "gensim_model.wv.save_word2vec_format(os.path.join(output_folder, \"ontology.embeddings.txt\"), binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e1f084-bf1f-44ac-8caf-c7f4e9d6880f",
   "metadata": {},
   "source": [
    "#### Embeddings check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471bcc2c-22cb-494b-96a8-c15c60787a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embedding vectors generated above\n",
    "model = KeyedVectors.load(output_folder+\"ontology.embeddings\", mmap='r')\n",
    "wv = model.wv\n",
    "\n",
    "word = 'http://www.orpha.net/ORDO/Orphanet_556985'\n",
    "vector = wv[word]  # Get numpy vector of a word\n",
    "print(f\"Vector for {word}: {vector}\")\n",
    "\n",
    "#Most similar cosine similarity\n",
    "result1 = wv.most_similar(positive=[word])\n",
    "print(result1)\n",
    "\n",
    "#Most similar entities: cosmul\n",
    "result2 = wv.most_similar_cosmul(positive=[word])\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e914991-f997-4f20-b899-3eb7073c14cd",
   "metadata": {},
   "source": [
    "### Sample synthetic data from the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9b976c-8a5a-4997-9f45-43451ea9379a",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "nb_rows = 600\n",
    "model_file = '../persistent/model/leukemia_600_50_5000_epochs_onto_dp_cgans_model.pkl'\n",
    "seen_save_path = f'../persistent/model/{current_time}_seen_sample_{nb_rows}_rows.csv'\n",
    "unseen_save_path = f'../persistent/model/{current_time}_unseen_sample_{nb_rows}_rows.csv'\n",
    "\n",
    "model = SDV.load(model_file)\n",
    "\n",
    "# Unseen ZSL sampling\n",
    "unseen_file = '../persistent/data/syn_data/unseen_rds_3_leukemia.txt'\n",
    "picked_unseen_rds = []\n",
    "with open(unseen_file) as uf:\n",
    "    for rd in uf:\n",
    "        picked_unseen_rds.append(rd.strip())\n",
    "\n",
    "print(f'Sampling {nb_rows} seen rows')\n",
    "model.sample(nb_rows).to_csv(seen_save_path)\n",
    "print(f'Sampling {nb_rows} unseen rows')\n",
    "model.sample(nb_rows, unseen_rds=picked_unseen_rds).to_csv(unseen_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939981bd-c42b-408f-a6f3-fc40fd72f624",
   "metadata": {},
   "source": [
    "### Evaluate generated datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f92d31-8aa4-499a-b685-7a40da86a065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingore pandas and python warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "\n",
    "def print_similarity(title, fsp_fn, fup_fn, ssp_fn, sup_fn):\n",
    "    \"\"\"Compute various metrics to evaluate the generated datasets.\n",
    "\n",
    "    Args:\n",
    "        title (str):\n",
    "            Info about the dataset and training.\n",
    "        fsp_fn (str):\n",
    "            Fake seen patients dataset (generated from the ontology).\n",
    "        fup_fn (str):\n",
    "            Fake unseen patients dataset (generated from the ontology).\n",
    "        ssp_fn (str):\n",
    "            Sampled seen patients dataset (sampled from the model).\n",
    "        sup_fn (str):\n",
    "            Sampled unseen patients dataset (sampled from the model).\n",
    "    \"\"\"\n",
    "    sampled_seen_patients = pd.read_csv(ssp_fn)\n",
    "    sampled_seen_patients.drop(columns=sampled_seen_patients.columns[0], axis=1, inplace=True)\n",
    "    sampled_unseen_patients = pd.read_csv(sup_fn)\n",
    "    sampled_unseen_patients.drop(columns=sampled_unseen_patients.columns[0], axis=1, inplace=True)\n",
    "    fake_seen_patients = pd.read_csv(fsp_fn)\n",
    "    fake_unseen_patients = pd.read_csv(fup_fn)\n",
    "    # Aligning the fake unseen datasets' columns with the columns of the unseen sampled dataset\n",
    "    fake_unseen_patients = fake_unseen_patients[sampled_unseen_patients.columns]\n",
    "\n",
    "    print(f'Results for {title}')\n",
    "    print(f'Fake Seen Patients: {len(fake_seen_patients)}x{len(fake_seen_patients.columns)} Fake Unseen Patients: {len(fake_unseen_patients)}x{len(fake_unseen_patients.columns)} Sampled Seen Patients: {len(sampled_seen_patients)}x{len(sampled_seen_patients.columns)} Sampled Unseen Patients: {len(sampled_unseen_patients)}x{len(sampled_unseen_patients.columns)}')\n",
    "    print(f'Similarity between fake and sampled seen patients data: {evaluate(fake_seen_patients, sampled_seen_patients):.3f}')\n",
    "    print(f'Similarity between fake and sampled unseen patients data: {evaluate(fake_unseen_patients, sampled_unseen_patients):.3f}')\n",
    "\n",
    "    for (text, fp, sp) in [('Seen', fake_seen_patients, sampled_seen_patients), ('Unseen', fake_unseen_patients, sampled_unseen_patients)]:\n",
    "        print(f\"CSTest, {text}: {CSTest.compute(fp, sp):.3f}\")\n",
    "        print(f\"KSTest, {text}: {KSTest.compute(fp, sp):.3f}\")\n",
    "        print(f\"BDT, {text}: {BinaryDecisionTreeClassifier.compute(fp, sp, target='rare_disease'):.3f}\")\n",
    "        print(f\"Ada, {text}: {BinaryAdaBoostClassifier.compute(fp, sp, target='rare_disease'):.3f}\")\n",
    "        print(f\"LR, {text}: {BinaryLogisticRegression.compute(fp, sp, target='rare_disease'):.3f}\")\n",
    "        print(f\"MLP, {text}: {BinaryMLPClassifier.compute(fp, sp, target='rare_disease'):.3f}\")\n",
    "        # print(f\"BN, {text}: {BNLikelihood.compute(fp, sp):.3f}\")\n",
    "        print(f\"LD, {text}: {LogisticDetection.compute(fp, sp):.3f}\")\n",
    "\n",
    "    print('')  # line break\n",
    "\n",
    "\n",
    "print_similarity(title='Brain and lung RDs, 100x50 dataset, 5000 epochs',\n",
    "                 fsp_fn='../persistent/data/syn_data/syn_patients_data_seen_100_50_brain_lung.csv',\n",
    "                 fup_fn='../persistent/data/syn_data/syn_patients_data_unseen_100_50_brain_lung (353).csv',\n",
    "                 ssp_fn='../persistent/model/brain_lung_100_50_seen_sample_100_rows.csv',\n",
    "                 sup_fn='../persistent/model/brain_lung_100_50_unseen_sample_100_rows.csv'\n",
    "                )\n",
    "\n",
    "print_similarity(title='Leukemia RDs, 600x50 dataset, 5000 epochs',\n",
    "                 fsp_fn='../persistent/data/syn_data/syn_patients_data_seen_600_50_leukemia.csv',\n",
    "                 fup_fn='../persistent/data/syn_data/syn_patients_data_unseen_150_50_leukemia.csv',\n",
    "                 ssp_fn='../persistent/model/2022_08_23_13_23_06_seen_sample_600_rows.csv',\n",
    "                 sup_fn='../persistent/model/2022_08_23_13_23_06_unseen_sample_600_rows.csv'\n",
    "                )\n",
    "\n",
    "print_similarity(title='Leukemia RDs, 600x50 dataset, 10000 epochs',\n",
    "                 fsp_fn='../persistent/data/syn_data/syn_patients_data_seen_600_50_leukemia.csv',\n",
    "                 fup_fn='../persistent/data/syn_data/syn_patients_data_unseen_150_50_leukemia.csv',\n",
    "                 ssp_fn='../persistent/model/leukemia_600_50_10000_epochs_seen_sample_600_rows.csv',\n",
    "                 sup_fn='../persistent/model/leukemia_600_50_10000_epochs_unseen_sample_600_rows.csv'\n",
    "                )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
