{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e378be34",
   "metadata": {},
   "source": [
    "## Code from OntoZSL (TransE embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a5938a-7cc4-4a47-8a85-abb1a7001df9",
   "metadata": {},
   "source": [
    "Used for comparison with OWL2Vec* embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba349da4",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec2fc82e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, triples, nentity, nrelation, negative_sample_size, mode):\n",
    "        self.len = len(triples)\n",
    "        self.triples = triples\n",
    "        self.triple_set = set(triples)\n",
    "        self.nentity = nentity\n",
    "        self.nrelation = nrelation\n",
    "        self.negative_sample_size = negative_sample_size\n",
    "        self.mode = mode\n",
    "        self.count = self.count_frequency(triples)\n",
    "        self.true_head, self.true_tail = self.get_true_head_and_tail(self.triples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        positive_sample = self.triples[idx]\n",
    "\n",
    "        head, relation, tail = positive_sample\n",
    "\n",
    "        subsampling_weight = self.count[(head, relation)] + self.count[(tail, -relation - 1)]\n",
    "        subsampling_weight = torch.sqrt(1 / torch.Tensor([subsampling_weight]))\n",
    "\n",
    "        negative_sample_list = []\n",
    "        negative_sample_size = 0\n",
    "\n",
    "        while negative_sample_size < self.negative_sample_size:\n",
    "            negative_sample = np.random.randint(self.nentity, size=self.negative_sample_size * 2)\n",
    "            if self.mode == 'head-batch':\n",
    "                mask = np.in1d(\n",
    "                    negative_sample,\n",
    "                    self.true_head[(relation, tail)],\n",
    "                    assume_unique=True,\n",
    "                    invert=True\n",
    "                )\n",
    "            elif self.mode == 'tail-batch':\n",
    "                mask = np.in1d(\n",
    "                    negative_sample,\n",
    "                    self.true_tail[(head, relation)],\n",
    "                    assume_unique=True,\n",
    "                    invert=True\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError('Training batch mode %s not supported' % self.mode)\n",
    "            negative_sample = negative_sample[mask]\n",
    "            negative_sample_list.append(negative_sample)\n",
    "            negative_sample_size += negative_sample.size\n",
    "\n",
    "        negative_sample = np.concatenate(negative_sample_list)[:self.negative_sample_size]\n",
    "\n",
    "        negative_sample = torch.from_numpy(negative_sample)\n",
    "\n",
    "        positive_sample = torch.LongTensor(positive_sample)\n",
    "\n",
    "        return positive_sample, negative_sample, subsampling_weight, self.mode\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(data):\n",
    "        positive_sample = torch.stack([_[0] for _ in data], dim=0)\n",
    "        negative_sample = torch.stack([_[1] for _ in data], dim=0)\n",
    "        subsample_weight = torch.cat([_[2] for _ in data], dim=0)\n",
    "        mode = data[0][3]\n",
    "        return positive_sample, negative_sample, subsample_weight, mode\n",
    "\n",
    "    @staticmethod\n",
    "    def count_frequency(triples, start=4):\n",
    "        '''\n",
    "        Get frequency of a partial triple like (head, relation) or (relation, tail)\n",
    "        The frequency will be used for subsampling like word2vec\n",
    "        '''\n",
    "        count = {}\n",
    "        for head, relation, tail in triples:\n",
    "            if (head, relation) not in count:\n",
    "                count[(head, relation)] = start\n",
    "            else:\n",
    "                count[(head, relation)] += 1\n",
    "\n",
    "            if (tail, -relation - 1) not in count:\n",
    "                count[(tail, -relation - 1)] = start\n",
    "            else:\n",
    "                count[(tail, -relation - 1)] += 1\n",
    "        return count\n",
    "\n",
    "    @staticmethod\n",
    "    def get_true_head_and_tail(triples):\n",
    "        '''\n",
    "        Build a dictionary of true triples that will\n",
    "        be used to filter these true triples for negative sampling\n",
    "        '''\n",
    "\n",
    "        true_head = {}\n",
    "        true_tail = {}\n",
    "\n",
    "        for head, relation, tail in triples:\n",
    "            if (head, relation) not in true_tail:\n",
    "                true_tail[(head, relation)] = []\n",
    "            true_tail[(head, relation)].append(tail)\n",
    "            if (relation, tail) not in true_head:\n",
    "                true_head[(relation, tail)] = []\n",
    "            true_head[(relation, tail)].append(head)\n",
    "\n",
    "        for relation, tail in true_head:\n",
    "            true_head[(relation, tail)] = np.array(list(set(true_head[(relation, tail)])))\n",
    "        for head, relation in true_tail:\n",
    "            true_tail[(head, relation)] = np.array(list(set(true_tail[(head, relation)])))\n",
    "\n",
    "        return true_head, true_tail\n",
    "\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, triples, all_true_triples, nentity, nrelation, mode):\n",
    "        self.len = len(triples)\n",
    "        self.triple_set = set(all_true_triples)\n",
    "        self.triples = triples\n",
    "        self.nentity = nentity\n",
    "        self.nrelation = nrelation\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        head, relation, tail = self.triples[idx]\n",
    "\n",
    "        if self.mode == 'head-batch':\n",
    "            tmp = [(0, rand_head) if (rand_head, relation, tail) not in self.triple_set\n",
    "                   else (-1, head) for rand_head in range(self.nentity)]\n",
    "            tmp[head] = (0, head)\n",
    "        elif self.mode == 'tail-batch':\n",
    "            tmp = [(0, rand_tail) if (head, relation, rand_tail) not in self.triple_set\n",
    "                   else (-1, tail) for rand_tail in range(self.nentity)]\n",
    "            tmp[tail] = (0, tail)\n",
    "        else:\n",
    "            raise ValueError('negative batch mode %s not supported' % self.mode)\n",
    "\n",
    "        tmp = torch.LongTensor(tmp)\n",
    "        filter_bias = tmp[:, 0].float()\n",
    "        negative_sample = tmp[:, 1]\n",
    "\n",
    "        positive_sample = torch.LongTensor((head, relation, tail))\n",
    "\n",
    "        return positive_sample, negative_sample, filter_bias, self.mode\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(data):\n",
    "        positive_sample = torch.stack([_[0] for _ in data], dim=0)\n",
    "        negative_sample = torch.stack([_[1] for _ in data], dim=0)\n",
    "        filter_bias = torch.stack([_[2] for _ in data], dim=0)\n",
    "        mode = data[0][3]\n",
    "        return positive_sample, negative_sample, filter_bias, mode\n",
    "\n",
    "\n",
    "class BidirectionalOneShotIterator(object):\n",
    "    def __init__(self, dataloader_head, dataloader_tail):\n",
    "        self.iterator_head = self.one_shot_iterator(dataloader_head)\n",
    "        self.iterator_tail = self.one_shot_iterator(dataloader_tail)\n",
    "        self.step = 0\n",
    "\n",
    "    def __next__(self):\n",
    "        self.step += 1\n",
    "        if self.step % 2 == 0:\n",
    "            data = next(self.iterator_head)\n",
    "        else:\n",
    "            data = next(self.iterator_tail)\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def one_shot_iterator(dataloader):\n",
    "        '''\n",
    "        Transform a PyTorch Dataloader into python iterator\n",
    "        '''\n",
    "        while True:\n",
    "            for data in dataloader:\n",
    "                yield data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a5f1e6",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0dddd25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class KGEModel(nn.Module):\n",
    "    def __init__(self, model_name, nentity, nrelation, hidden_dim, gamma,\n",
    "                 double_entity_embedding=False, double_relation_embedding=False):\n",
    "        super(KGEModel, self).__init__()\n",
    "        self.model_name = model_name\n",
    "        self.nentity = nentity\n",
    "        self.nrelation = nrelation\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.epsilon = 2.0\n",
    "\n",
    "        self.gamma = nn.Parameter(\n",
    "            torch.Tensor([gamma]),\n",
    "            requires_grad=False\n",
    "        )\n",
    "\n",
    "        self.embedding_range = nn.Parameter(\n",
    "            torch.Tensor([(self.gamma.item() + self.epsilon) / hidden_dim]),\n",
    "            requires_grad=False\n",
    "        )\n",
    "\n",
    "        self.entity_dim = hidden_dim * 2 if double_entity_embedding else hidden_dim\n",
    "        self.relation_dim = hidden_dim * 2 if double_relation_embedding else hidden_dim\n",
    "\n",
    "        self.entity_embedding = nn.Parameter(torch.zeros(nentity, self.entity_dim))\n",
    "        nn.init.uniform_(\n",
    "            tensor=self.entity_embedding,\n",
    "            a=-self.embedding_range.item(),\n",
    "            b=self.embedding_range.item()\n",
    "        )\n",
    "\n",
    "        self.relation_embedding = nn.Parameter(torch.zeros(nrelation, self.relation_dim))\n",
    "        nn.init.uniform_(\n",
    "            tensor=self.relation_embedding,\n",
    "            a=-self.embedding_range.item(),\n",
    "            b=self.embedding_range.item()\n",
    "        )\n",
    "\n",
    "        if model_name == 'pRotatE':\n",
    "            self.modulus = nn.Parameter(torch.Tensor([[0.5 * self.embedding_range.item()]]))\n",
    "\n",
    "        # Do not forget to modify this line when you add a new model in the \"forward\" function\n",
    "        if model_name not in ['TransE', 'DistMult', 'ComplEx', 'RotatE', 'pRotatE']:\n",
    "            raise ValueError('model %s not supported' % model_name)\n",
    "\n",
    "        if model_name == 'RotatE' and (not double_entity_embedding or double_relation_embedding):\n",
    "            raise ValueError('RotatE should use --double_entity_embedding')\n",
    "\n",
    "        if model_name == 'ComplEx' and (not double_entity_embedding or not double_relation_embedding):\n",
    "            raise ValueError('ComplEx should use --double_entity_embedding and --double_relation_embedding')\n",
    "\n",
    "    def forward(self, sample, mode='single'):\n",
    "        '''\n",
    "        Forward function that calculate the score of a batch of triples.\n",
    "        In the 'single' mode, sample is a batch of triple.\n",
    "        In the 'head-batch' or 'tail-batch' mode, sample consists two part.\n",
    "        The first part is usually the positive sample.\n",
    "        And the second part is the entities in the negative samples.\n",
    "        Because negative samples and positive samples usually share two elements\n",
    "        in their triple ((head, relation) or (relation, tail)).\n",
    "        '''\n",
    "\n",
    "        if mode == 'single':\n",
    "            batch_size, negative_sample_size = sample.size(0), 1\n",
    "\n",
    "            head = torch.index_select(\n",
    "                self.entity_embedding,\n",
    "                dim=0,\n",
    "                index=sample[:, 0]\n",
    "            ).unsqueeze(1)\n",
    "\n",
    "            relation = torch.index_select(\n",
    "                self.relation_embedding,\n",
    "                dim=0,\n",
    "                index=sample[:, 1]\n",
    "            ).unsqueeze(1)\n",
    "\n",
    "            tail = torch.index_select(\n",
    "                self.entity_embedding,\n",
    "                dim=0,\n",
    "                index=sample[:, 2]\n",
    "            ).unsqueeze(1)\n",
    "        elif mode == 'head-batch':\n",
    "            tail_part, head_part = sample\n",
    "            batch_size, negative_sample_size = head_part.size(0), head_part.size(1)\n",
    "\n",
    "            head = torch.index_select(\n",
    "                self.entity_embedding,\n",
    "                dim=0,\n",
    "                index=head_part.view(-1)\n",
    "            ).view(batch_size, negative_sample_size, -1)\n",
    "\n",
    "            relation = torch.index_select(\n",
    "                self.relation_embedding,\n",
    "                dim=0,\n",
    "                index=tail_part[:, 1]\n",
    "            ).unsqueeze(1)\n",
    "\n",
    "            tail = torch.index_select(\n",
    "                self.entity_embedding,\n",
    "                dim=0,\n",
    "                index=tail_part[:, 2]\n",
    "            ).unsqueeze(1)\n",
    "\n",
    "        elif mode == 'tail-batch':\n",
    "            head_part, tail_part = sample\n",
    "            batch_size, negative_sample_size = tail_part.size(0), tail_part.size(1)\n",
    "\n",
    "            head = torch.index_select(\n",
    "                self.entity_embedding,\n",
    "                dim=0,\n",
    "                index=head_part[:, 0]\n",
    "            ).unsqueeze(1)\n",
    "\n",
    "            relation = torch.index_select(\n",
    "                self.relation_embedding,\n",
    "                dim=0,\n",
    "                index=head_part[:, 1]\n",
    "            ).unsqueeze(1)\n",
    "\n",
    "            tail = torch.index_select(\n",
    "                self.entity_embedding,\n",
    "                dim=0,\n",
    "                index=tail_part.view(-1)\n",
    "            ).view(batch_size, negative_sample_size, -1)\n",
    "\n",
    "        else:\n",
    "            raise ValueError('mode %s not supported' % mode)\n",
    "\n",
    "        model_func = {\n",
    "            'TransE': self.TransE,\n",
    "            'DistMult': self.DistMult,\n",
    "            'ComplEx': self.ComplEx,\n",
    "            'RotatE': self.RotatE,\n",
    "            'pRotatE': self.pRotatE\n",
    "        }\n",
    "\n",
    "        if self.model_name in model_func:\n",
    "            score = model_func[self.model_name](head, relation, tail, mode)\n",
    "        else:\n",
    "            raise ValueError('model %s not supported' % self.model_name)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def TransE(self, head, relation, tail, mode):\n",
    "\n",
    "        if mode == 'head-batch':\n",
    "            score = head + (relation - tail)\n",
    "        else:\n",
    "            score = (head + relation) - tail\n",
    "\n",
    "        score = self.gamma.item() - torch.norm(score, p=1, dim=2)\n",
    "        return score\n",
    "\n",
    "    def DistMult(self, head, relation, tail, mode):\n",
    "        if mode == 'head-batch':\n",
    "            score = head * (relation * tail)\n",
    "        else:\n",
    "            score = (head * relation) * tail\n",
    "\n",
    "        score = score.sum(dim=2)\n",
    "        return score\n",
    "\n",
    "    def ComplEx(self, head, relation, tail, mode):\n",
    "        re_head, im_head = torch.chunk(head, 2, dim=2)\n",
    "        re_relation, im_relation = torch.chunk(relation, 2, dim=2)\n",
    "        re_tail, im_tail = torch.chunk(tail, 2, dim=2)\n",
    "\n",
    "        if mode == 'head-batch':\n",
    "            re_score = re_relation * re_tail + im_relation * im_tail\n",
    "            im_score = re_relation * im_tail - im_relation * re_tail\n",
    "            score = re_head * re_score + im_head * im_score\n",
    "        else:\n",
    "            re_score = re_head * re_relation - im_head * im_relation\n",
    "            im_score = re_head * im_relation + im_head * re_relation\n",
    "            score = re_score * re_tail + im_score * im_tail\n",
    "\n",
    "        score = score.sum(dim=2)\n",
    "        return score\n",
    "\n",
    "    def RotatE(self, head, relation, tail, mode):\n",
    "        pi = 3.14159265358979323846\n",
    "\n",
    "        re_head, im_head = torch.chunk(head, 2, dim=2)\n",
    "        re_tail, im_tail = torch.chunk(tail, 2, dim=2)\n",
    "\n",
    "        # Make phases of relations uniformly distributed in [-pi, pi]\n",
    "\n",
    "        phase_relation = relation / (self.embedding_range.item() / pi)\n",
    "\n",
    "        re_relation = torch.cos(phase_relation)\n",
    "        im_relation = torch.sin(phase_relation)\n",
    "\n",
    "        if mode == 'head-batch':\n",
    "            re_score = re_relation * re_tail + im_relation * im_tail\n",
    "            im_score = re_relation * im_tail - im_relation * re_tail\n",
    "            re_score = re_score - re_head\n",
    "            im_score = im_score - im_head\n",
    "        else:\n",
    "            re_score = re_head * re_relation - im_head * im_relation\n",
    "            im_score = re_head * im_relation + im_head * re_relation\n",
    "            re_score = re_score - re_tail\n",
    "            im_score = im_score - im_tail\n",
    "\n",
    "        score = torch.stack([re_score, im_score], dim=0)\n",
    "        score = score.norm(dim=0)\n",
    "\n",
    "        score = self.gamma.item() - score.sum(dim=2)\n",
    "        return score\n",
    "\n",
    "    def pRotatE(self, head, relation, tail, mode):\n",
    "        pi = 3.14159262358979323846\n",
    "\n",
    "        # Make phases of entities and relations uniformly distributed in [-pi, pi]\n",
    "\n",
    "        phase_head = head / (self.embedding_range.item() / pi)\n",
    "        phase_relation = relation / (self.embedding_range.item() / pi)\n",
    "        phase_tail = tail / (self.embedding_range.item() / pi)\n",
    "\n",
    "        if mode == 'head-batch':\n",
    "            score = phase_head + (phase_relation - phase_tail)\n",
    "        else:\n",
    "            score = (phase_head + phase_relation) - phase_tail\n",
    "\n",
    "        score = torch.sin(score)\n",
    "        score = torch.abs(score)\n",
    "\n",
    "        score = self.gamma.item() - score.sum(dim=2) * self.modulus\n",
    "        return score\n",
    "\n",
    "    @staticmethod\n",
    "    def train_step(model, optimizer, train_iterator, args):\n",
    "        '''\n",
    "        A single train step. Apply back-propation and return the loss\n",
    "        '''\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        positive_sample, negative_sample, subsampling_weight, mode = next(train_iterator)\n",
    "\n",
    "        if args.cuda:\n",
    "            positive_sample = positive_sample.cuda()\n",
    "            negative_sample = negative_sample.cuda()\n",
    "            subsampling_weight = subsampling_weight.cuda()\n",
    "\n",
    "        negative_score = model((positive_sample, negative_sample), mode=mode)\n",
    "\n",
    "        if args.negative_adversarial_sampling:\n",
    "            # In self-adversarial sampling, we do not apply back-propagation on the sampling weight\n",
    "            negative_score = (F.softmax(negative_score * args.adversarial_temperature, dim=1).detach()\n",
    "                              * F.logsigmoid(-negative_score)).sum(dim=1)\n",
    "        else:\n",
    "            negative_score = F.logsigmoid(-negative_score).mean(dim=1)\n",
    "\n",
    "        positive_score = model(positive_sample)\n",
    "\n",
    "        positive_score = F.logsigmoid(positive_score).squeeze(dim=1)\n",
    "\n",
    "        if args.uni_weight:\n",
    "            positive_sample_loss = - positive_score.mean()\n",
    "            negative_sample_loss = - negative_score.mean()\n",
    "        else:\n",
    "            positive_sample_loss = - (subsampling_weight * positive_score).sum() / subsampling_weight.sum()\n",
    "            negative_sample_loss = - (subsampling_weight * negative_score).sum() / subsampling_weight.sum()\n",
    "\n",
    "        loss = (positive_sample_loss + negative_sample_loss) / 2\n",
    "\n",
    "        if args.regularization != 0.0:\n",
    "            # Use L3 regularization for ComplEx and DistMult\n",
    "            regularization = args.regularization * (\n",
    "                    model.entity_embedding.norm(p=3) ** 3 +\n",
    "                    model.relation_embedding.norm(p=3).norm(p=3) ** 3\n",
    "            )\n",
    "            loss = loss + regularization\n",
    "            regularization_log = {'regularization': regularization.item()}\n",
    "        else:\n",
    "            regularization_log = {}\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_values = {\n",
    "            **regularization_log,\n",
    "            'pos_sample_loss': positive_sample_loss.item(),\n",
    "            'neg_sample_loss': negative_sample_loss.item(),\n",
    "            'loss': loss.item()\n",
    "        }\n",
    "\n",
    "        return loss_values\n",
    "\n",
    "    @staticmethod\n",
    "    def test_step(model, test_triples, all_true_triples, args):\n",
    "        '''\n",
    "        Evaluate the model on test or valid datasets\n",
    "        '''\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        # Otherwise use standard (filtered) MRR, MR, HITS@1, HITS@3, and HITS@10 metrics\n",
    "        # Prepare dataloader for evaluation\n",
    "        test_dataloader_head = DataLoader(\n",
    "            TestDataset(\n",
    "                test_triples,\n",
    "                all_true_triples,\n",
    "                args.nentity,\n",
    "                args.nrelation,\n",
    "                'head-batch'\n",
    "            ),\n",
    "            batch_size=args.test_batch_size,\n",
    "            num_workers=max(1, args.cpu_num // 2),\n",
    "            collate_fn=TestDataset.collate_fn\n",
    "        )\n",
    "\n",
    "        test_dataloader_tail = DataLoader(\n",
    "            TestDataset(\n",
    "                test_triples,\n",
    "                all_true_triples,\n",
    "                args.nentity,\n",
    "                args.nrelation,\n",
    "                'tail-batch'\n",
    "            ),\n",
    "            batch_size=args.test_batch_size,\n",
    "            num_workers=max(1, args.cpu_num // 2),\n",
    "            collate_fn=TestDataset.collate_fn\n",
    "        )\n",
    "\n",
    "        test_dataset_list = [test_dataloader_head, test_dataloader_tail]\n",
    "\n",
    "        logs = []\n",
    "\n",
    "        step = 0\n",
    "        total_steps = sum([len(dataset) for dataset in test_dataset_list])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for test_dataset in test_dataset_list:\n",
    "                for positive_sample, negative_sample, filter_bias, mode in test_dataset:\n",
    "                    if args.cuda:\n",
    "                        positive_sample = positive_sample.cuda()\n",
    "                        negative_sample = negative_sample.cuda()\n",
    "                        filter_bias = filter_bias.cuda()\n",
    "\n",
    "                    batch_size = positive_sample.size(0)\n",
    "\n",
    "                    score = model((positive_sample, negative_sample), mode)\n",
    "                    score += filter_bias\n",
    "\n",
    "                    # Explicitly sort all the entities to ensure that there is no test exposure bias\n",
    "                    argsort = torch.argsort(score, dim=1, descending=True)\n",
    "\n",
    "                    if mode == 'head-batch':\n",
    "                        positive_arg = positive_sample[:, 0]\n",
    "                    elif mode == 'tail-batch':\n",
    "                        positive_arg = positive_sample[:, 2]\n",
    "                    else:\n",
    "                        raise ValueError('mode %s not supported' % mode)\n",
    "\n",
    "                    for i in range(batch_size):\n",
    "                        # Notice that argsort is not ranking\n",
    "                        ranking = (argsort[i, :] == positive_arg[i]).nonzero()\n",
    "                        assert ranking.size(0) == 1\n",
    "\n",
    "                        # ranking + 1 is the true ranking used in evaluation metrics\n",
    "                        ranking = 1 + ranking.item()\n",
    "                        logs.append({\n",
    "                            'MRR': 1.0 / ranking,\n",
    "                            'MR': float(ranking),\n",
    "                            'HITS@1': 1.0 if ranking <= 1 else 0.0,\n",
    "                            'HITS@3': 1.0 if ranking <= 3 else 0.0,\n",
    "                            'HITS@10': 1.0 if ranking <= 10 else 0.0,\n",
    "                        })\n",
    "\n",
    "                    if step % args.test_log_steps == 0:\n",
    "                        logging.info('Evaluating the model... (%d/%d)' % (step, total_steps))\n",
    "\n",
    "                    step += 1\n",
    "\n",
    "        metrics = {}\n",
    "        for metric in logs[0].keys():\n",
    "            metrics[metric] = sum([log[metric] for log in logs]) / len(logs)\n",
    "\n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b65fe0f",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28e8efe2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import scipy.io as scio\n",
    "\n",
    "\n",
    "\n",
    "def readTxt(file_name):\n",
    "    class_list = list()\n",
    "    wnids = open(file_name, 'rU')\n",
    "    try:\n",
    "        for line in wnids:\n",
    "            line = line[:-1]\n",
    "            class_list.append(line)\n",
    "    finally:\n",
    "        wnids.close()\n",
    "    print(len(class_list))\n",
    "    return class_list\n",
    "\n",
    "def load_class():\n",
    "    seen = readTxt(seen_file)\n",
    "    unseen = readTxt(unseen_file)\n",
    "    return seen, unseen\n",
    "\n",
    "###########################\n",
    "\n",
    "def loadDict(file_name):\n",
    "    entities = list()\n",
    "    wnids = open(file_name, 'rU')\n",
    "    try:\n",
    "        for line in wnids:\n",
    "            line = line[:-1]\n",
    "            index, cls = line.split('\\t')\n",
    "            entities.append(cls)\n",
    "    finally:\n",
    "        wnids.close()\n",
    "    print(len(entities))\n",
    "    return entities\n",
    "\n",
    "\n",
    "def save_embed_awa(filename, wnids, names):\n",
    "\n",
    "    # load embeddings\n",
    "    embeds = np.load(filename)\n",
    "    # save to .mat file\n",
    "    matcontent = scio.loadmat(os.path.join(DATASET_DIR, 'att_splits.mat'))\n",
    "    all_names = matcontent['allclasses_names'].squeeze().tolist()\n",
    "\n",
    "    embed_size = embeds.shape[1]\n",
    "    o2v = np.zeros((len(all_names), embed_size), dtype=np.float)\n",
    "    for i in range(len(all_names)):\n",
    "        name = all_names[i][0]\n",
    "        wnid = wnids[names.index(name)]\n",
    "        o2v[i] = embeds[entities.index(wnid)]\n",
    "\n",
    "    print(o2v.shape)\n",
    "\n",
    "    o2v_file = os.path.join(DATA_DIR, save_file)\n",
    "    scio.savemat(o2v_file, {'o2v': o2v})\n",
    "\n",
    "def save_embed(filename, classes):\n",
    "\n",
    "    # load embeddings\n",
    "    embeds = np.load(filename)\n",
    "    # save to .mat file\n",
    "    matcontent = scio.loadmat(os.path.join(datadir, 'ImageNet', 'w2v.mat'))\n",
    "    wnids = matcontent['wnids'].squeeze().tolist()\n",
    "    wnids = wnids[:2549]\n",
    "    embed_size = embeds.shape[1]\n",
    "    o2v = np.zeros((len(wnids), embed_size), dtype=np.float)\n",
    "\n",
    "    print(o2v.shape)\n",
    "    for i, wnid in enumerate(wnids):\n",
    "        wnid = wnid[0]\n",
    "        if wnid in classes:\n",
    "            o2v[i] = embeds[entities.index(wnid)]\n",
    "        else:\n",
    "            continue\n",
    "    # save wnids together\n",
    "    wnids_cell = np.empty((len(wnids), 1), dtype=np.object)\n",
    "    for i in range(len(wnids)):\n",
    "        wnids_cell[i][0] = np.array(wnids[i])\n",
    "\n",
    "    o2v_file = os.path.join(DATA_DIR, save_file)\n",
    "    scio.savemat(o2v_file, {'o2v': o2v, 'wnids': wnids_cell})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8042697",
   "metadata": {},
   "source": [
    "### Training the model for the structural embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741a965c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: TransE\n",
      "#entity num: 124749\n",
      "#relation num: 5\n",
      "#total triples num: 573708\n",
      "Ramdomly Initializing TransE Model...\n",
      "------ Start Training...\n",
      "batch_size = 256\n",
      "negative sample size = 1024\n",
      "hidden_dim = 100\n",
      "gamma = 12.000000\n",
      "negative_adversarial_sampling = True\n",
      "adversarial_temperature = 1.000000\n",
      "learning rate = 0.000050\n",
      "Training Step: 100; average -> pos_sample_loss: 0.647852; neg_sample_loss: 1.166480; loss: 0.907166\n",
      "Training Step: 200; average -> pos_sample_loss: 0.738087; neg_sample_loss: 1.033381; loss: 0.885734\n",
      "Training Step: 300; average -> pos_sample_loss: 0.803018; neg_sample_loss: 0.946311; loss: 0.874665\n",
      "Training Step: 400; average -> pos_sample_loss: 0.826900; neg_sample_loss: 0.904552; loss: 0.865726\n",
      "Training Step: 500; average -> pos_sample_loss: 0.852270; neg_sample_loss: 0.876179; loss: 0.864225\n",
      "Training Step: 600; average -> pos_sample_loss: 0.837761; neg_sample_loss: 0.865987; loss: 0.851874\n",
      "Training Step: 700; average -> pos_sample_loss: 0.836540; neg_sample_loss: 0.860409; loss: 0.848475\n",
      "Training Step: 800; average -> pos_sample_loss: 0.845611; neg_sample_loss: 0.849311; loss: 0.847461\n",
      "Training Step: 900; average -> pos_sample_loss: 0.835892; neg_sample_loss: 0.850729; loss: 0.843311\n",
      "Training Step: 1000; average -> pos_sample_loss: 0.834362; neg_sample_loss: 0.847353; loss: 0.840857\n",
      "Training Step: 1100; average -> pos_sample_loss: 0.828549; neg_sample_loss: 0.848093; loss: 0.838321\n",
      "Training Step: 1200; average -> pos_sample_loss: 0.836816; neg_sample_loss: 0.845278; loss: 0.841047\n",
      "Training Step: 1300; average -> pos_sample_loss: 0.825703; neg_sample_loss: 0.843745; loss: 0.834724\n",
      "Training Step: 1400; average -> pos_sample_loss: 0.823363; neg_sample_loss: 0.843387; loss: 0.833375\n",
      "Training Step: 1500; average -> pos_sample_loss: 0.826535; neg_sample_loss: 0.842445; loss: 0.834490\n",
      "Training Step: 1600; average -> pos_sample_loss: 0.820732; neg_sample_loss: 0.843051; loss: 0.831892\n",
      "Training Step: 1700; average -> pos_sample_loss: 0.823264; neg_sample_loss: 0.836818; loss: 0.830041\n",
      "Training Step: 1800; average -> pos_sample_loss: 0.821021; neg_sample_loss: 0.835038; loss: 0.828029\n",
      "Training Step: 1900; average -> pos_sample_loss: 0.822065; neg_sample_loss: 0.837297; loss: 0.829681\n",
      "Training Step: 2000; average -> pos_sample_loss: 0.830941; neg_sample_loss: 0.831789; loss: 0.831365\n",
      "Training Step: 2100; average -> pos_sample_loss: 0.829333; neg_sample_loss: 0.833264; loss: 0.831299\n",
      "Training Step: 2200; average -> pos_sample_loss: 0.819226; neg_sample_loss: 0.833878; loss: 0.826552\n",
      "Training Step: 2300; average -> pos_sample_loss: 0.812234; neg_sample_loss: 0.829585; loss: 0.820909\n",
      "Training Step: 2400; average -> pos_sample_loss: 0.809384; neg_sample_loss: 0.830380; loss: 0.819882\n",
      "Training Step: 2500; average -> pos_sample_loss: 0.816718; neg_sample_loss: 0.825630; loss: 0.821174\n",
      "Training Step: 2600; average -> pos_sample_loss: 0.805454; neg_sample_loss: 0.826592; loss: 0.816023\n",
      "Training Step: 2700; average -> pos_sample_loss: 0.820358; neg_sample_loss: 0.821277; loss: 0.820818\n",
      "Training Step: 2800; average -> pos_sample_loss: 0.816711; neg_sample_loss: 0.820150; loss: 0.818430\n",
      "Training Step: 2900; average -> pos_sample_loss: 0.809982; neg_sample_loss: 0.823588; loss: 0.816785\n",
      "Training Step: 3000; average -> pos_sample_loss: 0.801350; neg_sample_loss: 0.820732; loss: 0.811041\n",
      "Training Step: 3100; average -> pos_sample_loss: 0.802708; neg_sample_loss: 0.817049; loss: 0.809879\n",
      "Training Step: 3200; average -> pos_sample_loss: 0.801535; neg_sample_loss: 0.814535; loss: 0.808035\n",
      "Training Step: 3300; average -> pos_sample_loss: 0.796586; neg_sample_loss: 0.807275; loss: 0.801930\n",
      "Training Step: 3400; average -> pos_sample_loss: 0.794645; neg_sample_loss: 0.809327; loss: 0.801986\n",
      "Training Step: 3500; average -> pos_sample_loss: 0.794974; neg_sample_loss: 0.804280; loss: 0.799627\n",
      "Training Step: 3600; average -> pos_sample_loss: 0.796939; neg_sample_loss: 0.806334; loss: 0.801636\n",
      "Training Step: 3700; average -> pos_sample_loss: 0.792795; neg_sample_loss: 0.801956; loss: 0.797376\n",
      "Training Step: 3800; average -> pos_sample_loss: 0.799323; neg_sample_loss: 0.799147; loss: 0.799235\n",
      "Training Step: 3900; average -> pos_sample_loss: 0.787690; neg_sample_loss: 0.800245; loss: 0.793967\n",
      "Training Step: 4000; average -> pos_sample_loss: 0.780595; neg_sample_loss: 0.801051; loss: 0.790823\n",
      "Training Step: 4100; average -> pos_sample_loss: 0.779243; neg_sample_loss: 0.795889; loss: 0.787566\n",
      "Training Step: 4200; average -> pos_sample_loss: 0.784800; neg_sample_loss: 0.790547; loss: 0.787673\n",
      "Training Step: 4300; average -> pos_sample_loss: 0.770321; neg_sample_loss: 0.797678; loss: 0.784000\n",
      "Training Step: 4400; average -> pos_sample_loss: 0.768877; neg_sample_loss: 0.792507; loss: 0.780692\n",
      "Training Step: 4500; average -> pos_sample_loss: 0.754131; neg_sample_loss: 0.789126; loss: 0.771628\n",
      "Training Step: 4600; average -> pos_sample_loss: 0.678774; neg_sample_loss: 0.783493; loss: 0.731133\n",
      "Training Step: 4700; average -> pos_sample_loss: 0.692525; neg_sample_loss: 0.765049; loss: 0.728787\n",
      "Training Step: 4800; average -> pos_sample_loss: 0.698673; neg_sample_loss: 0.754748; loss: 0.726711\n",
      "Training Step: 4900; average -> pos_sample_loss: 0.705981; neg_sample_loss: 0.749002; loss: 0.727491\n",
      "Training Step: 5000; average -> pos_sample_loss: 0.694193; neg_sample_loss: 0.745725; loss: 0.719959\n",
      "Training Step: 5100; average -> pos_sample_loss: 0.704257; neg_sample_loss: 0.745400; loss: 0.724829\n",
      "Training Step: 5200; average -> pos_sample_loss: 0.704582; neg_sample_loss: 0.742235; loss: 0.723408\n",
      "Training Step: 5300; average -> pos_sample_loss: 0.695126; neg_sample_loss: 0.742318; loss: 0.718722\n",
      "Training Step: 5400; average -> pos_sample_loss: 0.698822; neg_sample_loss: 0.738487; loss: 0.718654\n",
      "Training Step: 5500; average -> pos_sample_loss: 0.704125; neg_sample_loss: 0.737235; loss: 0.720680\n",
      "Training Step: 5600; average -> pos_sample_loss: 0.693087; neg_sample_loss: 0.737978; loss: 0.715532\n",
      "Training Step: 5700; average -> pos_sample_loss: 0.702452; neg_sample_loss: 0.734878; loss: 0.718665\n",
      "Training Step: 5800; average -> pos_sample_loss: 0.695468; neg_sample_loss: 0.733008; loss: 0.714238\n",
      "Training Step: 5900; average -> pos_sample_loss: 0.698044; neg_sample_loss: 0.730274; loss: 0.714159\n",
      "Training Step: 6000; average -> pos_sample_loss: 0.697107; neg_sample_loss: 0.731973; loss: 0.714540\n",
      "Training Step: 6100; average -> pos_sample_loss: 0.698318; neg_sample_loss: 0.729923; loss: 0.714120\n",
      "Training Step: 6200; average -> pos_sample_loss: 0.686762; neg_sample_loss: 0.728900; loss: 0.707831\n",
      "Training Step: 6300; average -> pos_sample_loss: 0.694467; neg_sample_loss: 0.726302; loss: 0.710384\n",
      "Training Step: 6400; average -> pos_sample_loss: 0.691236; neg_sample_loss: 0.725155; loss: 0.708196\n",
      "Training Step: 6500; average -> pos_sample_loss: 0.690793; neg_sample_loss: 0.724568; loss: 0.707681\n",
      "Training Step: 6600; average -> pos_sample_loss: 0.684420; neg_sample_loss: 0.723826; loss: 0.704123\n",
      "Training Step: 6700; average -> pos_sample_loss: 0.690306; neg_sample_loss: 0.720587; loss: 0.705446\n",
      "Training Step: 6800; average -> pos_sample_loss: 0.682570; neg_sample_loss: 0.724022; loss: 0.703296\n",
      "Training Step: 6900; average -> pos_sample_loss: 0.684062; neg_sample_loss: 0.717500; loss: 0.700781\n",
      "Training Step: 7000; average -> pos_sample_loss: 0.683073; neg_sample_loss: 0.715945; loss: 0.699509\n",
      "Training Step: 7100; average -> pos_sample_loss: 0.684429; neg_sample_loss: 0.715743; loss: 0.700086\n",
      "Training Step: 7200; average -> pos_sample_loss: 0.687470; neg_sample_loss: 0.716801; loss: 0.702136\n",
      "Training Step: 7300; average -> pos_sample_loss: 0.681591; neg_sample_loss: 0.714133; loss: 0.697862\n",
      "Training Step: 7400; average -> pos_sample_loss: 0.683629; neg_sample_loss: 0.715250; loss: 0.699440\n",
      "Training Step: 7500; average -> pos_sample_loss: 0.681407; neg_sample_loss: 0.713487; loss: 0.697447\n",
      "Training Step: 7600; average -> pos_sample_loss: 0.682892; neg_sample_loss: 0.711996; loss: 0.697444\n",
      "Training Step: 7700; average -> pos_sample_loss: 0.679033; neg_sample_loss: 0.710852; loss: 0.694943\n",
      "Training Step: 7800; average -> pos_sample_loss: 0.677351; neg_sample_loss: 0.710263; loss: 0.693807\n",
      "Training Step: 7900; average -> pos_sample_loss: 0.678921; neg_sample_loss: 0.708034; loss: 0.693477\n",
      "Training Step: 8000; average -> pos_sample_loss: 0.672699; neg_sample_loss: 0.707809; loss: 0.690254\n",
      "Training Step: 8100; average -> pos_sample_loss: 0.674969; neg_sample_loss: 0.708194; loss: 0.691582\n",
      "Training Step: 8200; average -> pos_sample_loss: 0.672274; neg_sample_loss: 0.708479; loss: 0.690377\n",
      "Training Step: 8300; average -> pos_sample_loss: 0.672399; neg_sample_loss: 0.704740; loss: 0.688570\n",
      "Training Step: 8400; average -> pos_sample_loss: 0.670653; neg_sample_loss: 0.699680; loss: 0.685166\n",
      "Training Step: 8500; average -> pos_sample_loss: 0.663312; neg_sample_loss: 0.703976; loss: 0.683644\n",
      "Training Step: 8600; average -> pos_sample_loss: 0.671097; neg_sample_loss: 0.702489; loss: 0.686793\n",
      "Training Step: 8700; average -> pos_sample_loss: 0.667345; neg_sample_loss: 0.701525; loss: 0.684435\n",
      "Training Step: 8800; average -> pos_sample_loss: 0.668014; neg_sample_loss: 0.697209; loss: 0.682612\n",
      "Training Step: 8900; average -> pos_sample_loss: 0.674053; neg_sample_loss: 0.696315; loss: 0.685184\n",
      "Training Step: 9000; average -> pos_sample_loss: 0.651023; neg_sample_loss: 0.697713; loss: 0.674368\n",
      "Training Step: 9100; average -> pos_sample_loss: 0.605377; neg_sample_loss: 0.691022; loss: 0.648200\n",
      "Training Step: 9200; average -> pos_sample_loss: 0.613868; neg_sample_loss: 0.681765; loss: 0.647817\n",
      "Training Step: 9300; average -> pos_sample_loss: 0.617106; neg_sample_loss: 0.672013; loss: 0.644559\n",
      "Training Step: 9400; average -> pos_sample_loss: 0.619139; neg_sample_loss: 0.671914; loss: 0.645526\n",
      "Training Step: 9500; average -> pos_sample_loss: 0.618602; neg_sample_loss: 0.670516; loss: 0.644559\n",
      "Training Step: 9600; average -> pos_sample_loss: 0.618453; neg_sample_loss: 0.670055; loss: 0.644254\n",
      "Training Step: 9700; average -> pos_sample_loss: 0.620537; neg_sample_loss: 0.665175; loss: 0.642856\n",
      "Training Step: 9800; average -> pos_sample_loss: 0.617827; neg_sample_loss: 0.664414; loss: 0.641120\n",
      "Training Step: 9900; average -> pos_sample_loss: 0.624361; neg_sample_loss: 0.664097; loss: 0.644229\n",
      "Training Step: 10000; average -> pos_sample_loss: 0.614815; neg_sample_loss: 0.667112; loss: 0.640964\n",
      "Training Step: 10100; average -> pos_sample_loss: 0.622520; neg_sample_loss: 0.664974; loss: 0.643747\n",
      "Training Step: 10200; average -> pos_sample_loss: 0.617166; neg_sample_loss: 0.661911; loss: 0.639538\n",
      "Training Step: 10300; average -> pos_sample_loss: 0.618939; neg_sample_loss: 0.660450; loss: 0.639694\n",
      "Training Step: 10400; average -> pos_sample_loss: 0.619052; neg_sample_loss: 0.661810; loss: 0.640431\n",
      "Training Step: 10500; average -> pos_sample_loss: 0.619480; neg_sample_loss: 0.662767; loss: 0.641124\n",
      "Training Step: 10600; average -> pos_sample_loss: 0.617246; neg_sample_loss: 0.659511; loss: 0.638378\n",
      "Training Step: 10700; average -> pos_sample_loss: 0.617908; neg_sample_loss: 0.659875; loss: 0.638891\n",
      "Training Step: 10800; average -> pos_sample_loss: 0.618226; neg_sample_loss: 0.657814; loss: 0.638020\n",
      "Training Step: 10900; average -> pos_sample_loss: 0.613093; neg_sample_loss: 0.660479; loss: 0.636786\n",
      "Training Step: 11000; average -> pos_sample_loss: 0.615181; neg_sample_loss: 0.659135; loss: 0.637158\n",
      "Training Step: 11100; average -> pos_sample_loss: 0.623619; neg_sample_loss: 0.653964; loss: 0.638791\n",
      "Training Step: 11200; average -> pos_sample_loss: 0.616403; neg_sample_loss: 0.655814; loss: 0.636109\n",
      "Training Step: 11300; average -> pos_sample_loss: 0.614294; neg_sample_loss: 0.656404; loss: 0.635349\n",
      "Training Step: 11400; average -> pos_sample_loss: 0.611795; neg_sample_loss: 0.653162; loss: 0.632479\n",
      "Training Step: 11500; average -> pos_sample_loss: 0.619592; neg_sample_loss: 0.651974; loss: 0.635783\n",
      "Training Step: 11600; average -> pos_sample_loss: 0.617110; neg_sample_loss: 0.654192; loss: 0.635651\n",
      "Training Step: 11700; average -> pos_sample_loss: 0.612778; neg_sample_loss: 0.654603; loss: 0.633691\n",
      "Training Step: 11800; average -> pos_sample_loss: 0.616605; neg_sample_loss: 0.651777; loss: 0.634191\n",
      "Training Step: 11900; average -> pos_sample_loss: 0.618738; neg_sample_loss: 0.649254; loss: 0.633996\n",
      "Training Step: 12000; average -> pos_sample_loss: 0.615172; neg_sample_loss: 0.653027; loss: 0.634099\n",
      "Training Step: 12100; average -> pos_sample_loss: 0.614534; neg_sample_loss: 0.651683; loss: 0.633109\n",
      "Training Step: 12200; average -> pos_sample_loss: 0.620182; neg_sample_loss: 0.649846; loss: 0.635014\n",
      "Training Step: 12300; average -> pos_sample_loss: 0.613450; neg_sample_loss: 0.650988; loss: 0.632219\n",
      "Training Step: 12400; average -> pos_sample_loss: 0.616720; neg_sample_loss: 0.650988; loss: 0.633854\n",
      "Training Step: 12500; average -> pos_sample_loss: 0.613317; neg_sample_loss: 0.647160; loss: 0.630238\n",
      "Training Step: 12600; average -> pos_sample_loss: 0.610148; neg_sample_loss: 0.649898; loss: 0.630023\n",
      "Training Step: 12700; average -> pos_sample_loss: 0.610978; neg_sample_loss: 0.647782; loss: 0.629380\n",
      "Training Step: 12800; average -> pos_sample_loss: 0.615275; neg_sample_loss: 0.639559; loss: 0.627417\n",
      "Training Step: 12900; average -> pos_sample_loss: 0.613628; neg_sample_loss: 0.644360; loss: 0.628994\n",
      "Training Step: 13000; average -> pos_sample_loss: 0.606983; neg_sample_loss: 0.647925; loss: 0.627454\n",
      "Training Step: 13100; average -> pos_sample_loss: 0.613836; neg_sample_loss: 0.640707; loss: 0.627271\n",
      "Training Step: 13200; average -> pos_sample_loss: 0.606559; neg_sample_loss: 0.640944; loss: 0.623752\n",
      "Training Step: 13300; average -> pos_sample_loss: 0.607229; neg_sample_loss: 0.642398; loss: 0.624814\n",
      "Training Step: 13400; average -> pos_sample_loss: 0.607514; neg_sample_loss: 0.644123; loss: 0.625819\n",
      "Training Step: 13500; average -> pos_sample_loss: 0.577732; neg_sample_loss: 0.642862; loss: 0.610297\n",
      "Training Step: 13600; average -> pos_sample_loss: 0.554255; neg_sample_loss: 0.632270; loss: 0.593262\n",
      "Training Step: 13700; average -> pos_sample_loss: 0.560837; neg_sample_loss: 0.624962; loss: 0.592899\n",
      "Training Step: 13800; average -> pos_sample_loss: 0.564209; neg_sample_loss: 0.621312; loss: 0.592760\n",
      "Training Step: 13900; average -> pos_sample_loss: 0.563302; neg_sample_loss: 0.621419; loss: 0.592361\n",
      "Training Step: 14000; average -> pos_sample_loss: 0.564154; neg_sample_loss: 0.617868; loss: 0.591011\n",
      "Training Step: 14100; average -> pos_sample_loss: 0.563763; neg_sample_loss: 0.616342; loss: 0.590052\n",
      "Training Step: 14200; average -> pos_sample_loss: 0.566665; neg_sample_loss: 0.615337; loss: 0.591001\n",
      "Training Step: 14300; average -> pos_sample_loss: 0.563740; neg_sample_loss: 0.616839; loss: 0.590290\n",
      "Training Step: 14400; average -> pos_sample_loss: 0.565394; neg_sample_loss: 0.618549; loss: 0.591972\n",
      "Training Step: 14500; average -> pos_sample_loss: 0.565682; neg_sample_loss: 0.613152; loss: 0.589417\n",
      "Training Step: 14600; average -> pos_sample_loss: 0.564745; neg_sample_loss: 0.612911; loss: 0.588828\n",
      "Training Step: 14700; average -> pos_sample_loss: 0.568342; neg_sample_loss: 0.612613; loss: 0.590477\n",
      "Training Step: 14800; average -> pos_sample_loss: 0.563909; neg_sample_loss: 0.614863; loss: 0.589386\n",
      "Training Step: 14900; average -> pos_sample_loss: 0.564067; neg_sample_loss: 0.611769; loss: 0.587918\n",
      "Training Step: 15000; average -> pos_sample_loss: 0.566889; neg_sample_loss: 0.612822; loss: 0.589855\n",
      "Training Step: 15100; average -> pos_sample_loss: 0.567553; neg_sample_loss: 0.607493; loss: 0.587523\n",
      "Training Step: 15200; average -> pos_sample_loss: 0.567761; neg_sample_loss: 0.610391; loss: 0.589076\n",
      "Training Step: 15300; average -> pos_sample_loss: 0.564821; neg_sample_loss: 0.612780; loss: 0.588800\n",
      "Training Step: 15400; average -> pos_sample_loss: 0.565679; neg_sample_loss: 0.611345; loss: 0.588512\n",
      "Training Step: 15500; average -> pos_sample_loss: 0.567681; neg_sample_loss: 0.609363; loss: 0.588522\n",
      "Training Step: 15600; average -> pos_sample_loss: 0.570164; neg_sample_loss: 0.607131; loss: 0.588648\n",
      "Training Step: 15700; average -> pos_sample_loss: 0.568884; neg_sample_loss: 0.607386; loss: 0.588135\n",
      "Training Step: 15800; average -> pos_sample_loss: 0.565382; neg_sample_loss: 0.610146; loss: 0.587764\n",
      "Training Step: 15900; average -> pos_sample_loss: 0.564702; neg_sample_loss: 0.606732; loss: 0.585717\n",
      "Training Step: 16000; average -> pos_sample_loss: 0.565043; neg_sample_loss: 0.606107; loss: 0.585575\n",
      "Training Step: 16100; average -> pos_sample_loss: 0.565709; neg_sample_loss: 0.605418; loss: 0.585563\n",
      "Training Step: 16200; average -> pos_sample_loss: 0.566908; neg_sample_loss: 0.604815; loss: 0.585862\n",
      "Training Step: 16300; average -> pos_sample_loss: 0.563357; neg_sample_loss: 0.606238; loss: 0.584797\n",
      "Training Step: 16400; average -> pos_sample_loss: 0.571075; neg_sample_loss: 0.602828; loss: 0.586952\n",
      "Training Step: 16500; average -> pos_sample_loss: 0.565831; neg_sample_loss: 0.602809; loss: 0.584320\n",
      "Training Step: 16600; average -> pos_sample_loss: 0.567061; neg_sample_loss: 0.602131; loss: 0.584596\n",
      "Training Step: 16700; average -> pos_sample_loss: 0.567814; neg_sample_loss: 0.604578; loss: 0.586196\n",
      "Training Step: 16800; average -> pos_sample_loss: 0.565469; neg_sample_loss: 0.606524; loss: 0.585997\n",
      "Training Step: 16900; average -> pos_sample_loss: 0.567483; neg_sample_loss: 0.600050; loss: 0.583767\n",
      "Training Step: 17000; average -> pos_sample_loss: 0.565480; neg_sample_loss: 0.599849; loss: 0.582664\n",
      "Training Step: 17100; average -> pos_sample_loss: 0.563750; neg_sample_loss: 0.601063; loss: 0.582406\n",
      "Training Step: 17200; average -> pos_sample_loss: 0.570621; neg_sample_loss: 0.600764; loss: 0.585693\n",
      "Training Step: 17300; average -> pos_sample_loss: 0.565513; neg_sample_loss: 0.600573; loss: 0.583043\n",
      "Training Step: 17400; average -> pos_sample_loss: 0.566175; neg_sample_loss: 0.599713; loss: 0.582944\n",
      "Training Step: 17500; average -> pos_sample_loss: 0.564666; neg_sample_loss: 0.599304; loss: 0.581985\n",
      "Training Step: 17600; average -> pos_sample_loss: 0.560793; neg_sample_loss: 0.598590; loss: 0.579691\n",
      "Training Step: 17700; average -> pos_sample_loss: 0.565522; neg_sample_loss: 0.598074; loss: 0.581798\n",
      "Training Step: 17800; average -> pos_sample_loss: 0.565139; neg_sample_loss: 0.598495; loss: 0.581817\n",
      "Training Step: 17900; average -> pos_sample_loss: 0.563414; neg_sample_loss: 0.594837; loss: 0.579126\n",
      "Training Step: 18000; average -> pos_sample_loss: 0.526611; neg_sample_loss: 0.595249; loss: 0.560930\n",
      "Training Step: 18100; average -> pos_sample_loss: 0.514886; neg_sample_loss: 0.589966; loss: 0.552426\n",
      "Training Step: 18200; average -> pos_sample_loss: 0.517873; neg_sample_loss: 0.583277; loss: 0.550575\n",
      "Training Step: 18300; average -> pos_sample_loss: 0.522327; neg_sample_loss: 0.581565; loss: 0.551946\n",
      "Training Step: 18400; average -> pos_sample_loss: 0.520177; neg_sample_loss: 0.577953; loss: 0.549065\n",
      "Training Step: 18500; average -> pos_sample_loss: 0.518896; neg_sample_loss: 0.579528; loss: 0.549212\n",
      "Training Step: 18600; average -> pos_sample_loss: 0.523873; neg_sample_loss: 0.576504; loss: 0.550189\n",
      "Training Step: 18700; average -> pos_sample_loss: 0.526079; neg_sample_loss: 0.575603; loss: 0.550841\n",
      "Training Step: 18800; average -> pos_sample_loss: 0.520178; neg_sample_loss: 0.574210; loss: 0.547194\n",
      "Training Step: 18900; average -> pos_sample_loss: 0.522199; neg_sample_loss: 0.575339; loss: 0.548769\n",
      "Training Step: 19000; average -> pos_sample_loss: 0.526952; neg_sample_loss: 0.574897; loss: 0.550924\n",
      "Training Step: 19100; average -> pos_sample_loss: 0.524596; neg_sample_loss: 0.575362; loss: 0.549979\n",
      "Training Step: 19200; average -> pos_sample_loss: 0.525034; neg_sample_loss: 0.572301; loss: 0.548667\n",
      "Training Step: 19300; average -> pos_sample_loss: 0.525963; neg_sample_loss: 0.571565; loss: 0.548764\n",
      "Training Step: 19400; average -> pos_sample_loss: 0.530741; neg_sample_loss: 0.570634; loss: 0.550687\n",
      "Training Step: 19500; average -> pos_sample_loss: 0.525486; neg_sample_loss: 0.569848; loss: 0.547667\n",
      "Training Step: 19600; average -> pos_sample_loss: 0.525144; neg_sample_loss: 0.572190; loss: 0.548667\n",
      "Training Step: 19700; average -> pos_sample_loss: 0.527249; neg_sample_loss: 0.569530; loss: 0.548390\n",
      "Training Step: 19800; average -> pos_sample_loss: 0.526900; neg_sample_loss: 0.569379; loss: 0.548139\n",
      "Training Step: 19900; average -> pos_sample_loss: 0.528416; neg_sample_loss: 0.572054; loss: 0.550235\n",
      "Training Step: 20000; average -> pos_sample_loss: 0.527178; neg_sample_loss: 0.568255; loss: 0.547716\n",
      "Training Step: 20100; average -> pos_sample_loss: 0.529323; neg_sample_loss: 0.568505; loss: 0.548914\n",
      "Training Step: 20200; average -> pos_sample_loss: 0.527689; neg_sample_loss: 0.570744; loss: 0.549217\n",
      "Training Step: 20300; average -> pos_sample_loss: 0.525030; neg_sample_loss: 0.567267; loss: 0.546148\n",
      "Training Step: 20400; average -> pos_sample_loss: 0.531378; neg_sample_loss: 0.568004; loss: 0.549691\n",
      "Training Step: 20500; average -> pos_sample_loss: 0.527926; neg_sample_loss: 0.565135; loss: 0.546531\n",
      "Training Step: 20600; average -> pos_sample_loss: 0.528775; neg_sample_loss: 0.563370; loss: 0.546073\n",
      "Training Step: 20700; average -> pos_sample_loss: 0.530056; neg_sample_loss: 0.565608; loss: 0.547832\n",
      "Training Step: 20800; average -> pos_sample_loss: 0.527242; neg_sample_loss: 0.566193; loss: 0.546717\n",
      "Training Step: 20900; average -> pos_sample_loss: 0.530333; neg_sample_loss: 0.564039; loss: 0.547186\n",
      "Training Step: 21000; average -> pos_sample_loss: 0.527447; neg_sample_loss: 0.564320; loss: 0.545884\n",
      "Training Step: 21100; average -> pos_sample_loss: 0.531754; neg_sample_loss: 0.563706; loss: 0.547730\n",
      "Training Step: 21200; average -> pos_sample_loss: 0.525719; neg_sample_loss: 0.565530; loss: 0.545624\n",
      "Training Step: 21300; average -> pos_sample_loss: 0.529512; neg_sample_loss: 0.560868; loss: 0.545190\n",
      "Training Step: 21400; average -> pos_sample_loss: 0.531933; neg_sample_loss: 0.563772; loss: 0.547853\n",
      "Training Step: 21500; average -> pos_sample_loss: 0.528518; neg_sample_loss: 0.563181; loss: 0.545849\n",
      "Training Step: 21600; average -> pos_sample_loss: 0.529248; neg_sample_loss: 0.563424; loss: 0.546336\n",
      "Training Step: 21700; average -> pos_sample_loss: 0.530570; neg_sample_loss: 0.563807; loss: 0.547189\n",
      "Training Step: 21800; average -> pos_sample_loss: 0.528666; neg_sample_loss: 0.562624; loss: 0.545645\n",
      "Training Step: 21900; average -> pos_sample_loss: 0.528078; neg_sample_loss: 0.562447; loss: 0.545263\n",
      "Training Step: 22000; average -> pos_sample_loss: 0.530439; neg_sample_loss: 0.558888; loss: 0.544664\n",
      "Training Step: 22100; average -> pos_sample_loss: 0.529413; neg_sample_loss: 0.563774; loss: 0.546593\n",
      "Training Step: 22200; average -> pos_sample_loss: 0.529210; neg_sample_loss: 0.559313; loss: 0.544261\n",
      "Training Step: 22300; average -> pos_sample_loss: 0.526034; neg_sample_loss: 0.559692; loss: 0.542863\n",
      "Training Step: 22400; average -> pos_sample_loss: 0.528431; neg_sample_loss: 0.560467; loss: 0.544449\n",
      "Training Step: 22500; average -> pos_sample_loss: 0.484089; neg_sample_loss: 0.559384; loss: 0.521737\n",
      "Training Step: 22600; average -> pos_sample_loss: 0.482292; neg_sample_loss: 0.547659; loss: 0.514976\n",
      "Training Step: 22700; average -> pos_sample_loss: 0.484525; neg_sample_loss: 0.544777; loss: 0.514651\n",
      "Training Step: 22800; average -> pos_sample_loss: 0.488251; neg_sample_loss: 0.543689; loss: 0.515970\n",
      "Training Step: 22900; average -> pos_sample_loss: 0.488360; neg_sample_loss: 0.543381; loss: 0.515871\n",
      "Training Step: 23000; average -> pos_sample_loss: 0.487994; neg_sample_loss: 0.540387; loss: 0.514190\n",
      "Training Step: 23100; average -> pos_sample_loss: 0.488464; neg_sample_loss: 0.542011; loss: 0.515237\n",
      "Training Step: 23200; average -> pos_sample_loss: 0.490068; neg_sample_loss: 0.540403; loss: 0.515236\n",
      "Training Step: 23300; average -> pos_sample_loss: 0.490529; neg_sample_loss: 0.540586; loss: 0.515558\n",
      "Training Step: 23400; average -> pos_sample_loss: 0.488573; neg_sample_loss: 0.541201; loss: 0.514887\n",
      "Training Step: 23500; average -> pos_sample_loss: 0.494379; neg_sample_loss: 0.539584; loss: 0.516981\n",
      "Training Step: 23600; average -> pos_sample_loss: 0.490347; neg_sample_loss: 0.536520; loss: 0.513434\n",
      "Training Step: 23700; average -> pos_sample_loss: 0.490982; neg_sample_loss: 0.538116; loss: 0.514549\n",
      "Training Step: 23800; average -> pos_sample_loss: 0.493467; neg_sample_loss: 0.541897; loss: 0.517682\n",
      "Training Step: 23900; average -> pos_sample_loss: 0.494346; neg_sample_loss: 0.538562; loss: 0.516454\n",
      "Training Step: 24000; average -> pos_sample_loss: 0.493674; neg_sample_loss: 0.537131; loss: 0.515403\n",
      "Training Step: 24100; average -> pos_sample_loss: 0.492412; neg_sample_loss: 0.537989; loss: 0.515201\n",
      "Training Step: 24200; average -> pos_sample_loss: 0.495391; neg_sample_loss: 0.537349; loss: 0.516370\n",
      "Training Step: 24300; average -> pos_sample_loss: 0.496487; neg_sample_loss: 0.534202; loss: 0.515345\n",
      "Training Step: 24400; average -> pos_sample_loss: 0.495244; neg_sample_loss: 0.534276; loss: 0.514760\n",
      "Training Step: 24500; average -> pos_sample_loss: 0.496251; neg_sample_loss: 0.532917; loss: 0.514584\n",
      "Training Step: 24600; average -> pos_sample_loss: 0.496166; neg_sample_loss: 0.533650; loss: 0.514908\n",
      "Training Step: 24700; average -> pos_sample_loss: 0.494488; neg_sample_loss: 0.535040; loss: 0.514764\n",
      "Training Step: 24800; average -> pos_sample_loss: 0.497618; neg_sample_loss: 0.533279; loss: 0.515448\n",
      "Training Step: 24900; average -> pos_sample_loss: 0.498483; neg_sample_loss: 0.535742; loss: 0.517113\n",
      "Training Step: 25000; average -> pos_sample_loss: 0.495042; neg_sample_loss: 0.531853; loss: 0.513447\n",
      "Training Step: 25100; average -> pos_sample_loss: 0.497125; neg_sample_loss: 0.531475; loss: 0.514300\n",
      "Training Step: 25200; average -> pos_sample_loss: 0.497805; neg_sample_loss: 0.530678; loss: 0.514241\n",
      "Training Step: 25300; average -> pos_sample_loss: 0.498363; neg_sample_loss: 0.531871; loss: 0.515117\n",
      "Training Step: 25400; average -> pos_sample_loss: 0.495226; neg_sample_loss: 0.534277; loss: 0.514751\n",
      "Training Step: 25500; average -> pos_sample_loss: 0.502154; neg_sample_loss: 0.529929; loss: 0.516041\n",
      "Training Step: 25600; average -> pos_sample_loss: 0.495362; neg_sample_loss: 0.531099; loss: 0.513230\n",
      "Training Step: 25700; average -> pos_sample_loss: 0.499355; neg_sample_loss: 0.528777; loss: 0.514066\n",
      "Training Step: 25800; average -> pos_sample_loss: 0.498288; neg_sample_loss: 0.530103; loss: 0.514195\n",
      "Training Step: 25900; average -> pos_sample_loss: 0.497113; neg_sample_loss: 0.530253; loss: 0.513683\n",
      "Training Step: 26000; average -> pos_sample_loss: 0.500515; neg_sample_loss: 0.529819; loss: 0.515167\n",
      "Training Step: 26100; average -> pos_sample_loss: 0.500339; neg_sample_loss: 0.528013; loss: 0.514176\n",
      "Training Step: 26200; average -> pos_sample_loss: 0.501691; neg_sample_loss: 0.531313; loss: 0.516502\n",
      "Training Step: 26300; average -> pos_sample_loss: 0.501577; neg_sample_loss: 0.527533; loss: 0.514555\n",
      "Training Step: 26400; average -> pos_sample_loss: 0.501090; neg_sample_loss: 0.527061; loss: 0.514076\n",
      "Training Step: 26500; average -> pos_sample_loss: 0.499882; neg_sample_loss: 0.527741; loss: 0.513811\n",
      "Training Step: 26600; average -> pos_sample_loss: 0.497058; neg_sample_loss: 0.528349; loss: 0.512703\n",
      "Training Step: 26700; average -> pos_sample_loss: 0.498956; neg_sample_loss: 0.525713; loss: 0.512334\n",
      "Training Step: 26800; average -> pos_sample_loss: 0.499106; neg_sample_loss: 0.526608; loss: 0.512857\n",
      "Training Step: 26900; average -> pos_sample_loss: 0.498541; neg_sample_loss: 0.525863; loss: 0.512202\n",
      "Training Step: 27000; average -> pos_sample_loss: 0.449257; neg_sample_loss: 0.526526; loss: 0.487891\n",
      "Training Step: 27100; average -> pos_sample_loss: 0.454520; neg_sample_loss: 0.516513; loss: 0.485516\n",
      "Training Step: 27200; average -> pos_sample_loss: 0.458542; neg_sample_loss: 0.513813; loss: 0.486178\n",
      "Training Step: 27300; average -> pos_sample_loss: 0.459476; neg_sample_loss: 0.512212; loss: 0.485844\n",
      "Training Step: 27400; average -> pos_sample_loss: 0.460110; neg_sample_loss: 0.513616; loss: 0.486863\n",
      "Training Step: 27500; average -> pos_sample_loss: 0.461718; neg_sample_loss: 0.510835; loss: 0.486276\n",
      "Training Step: 27600; average -> pos_sample_loss: 0.460457; neg_sample_loss: 0.511432; loss: 0.485945\n",
      "Training Step: 27700; average -> pos_sample_loss: 0.461979; neg_sample_loss: 0.509199; loss: 0.485589\n",
      "Training Step: 27800; average -> pos_sample_loss: 0.461519; neg_sample_loss: 0.510768; loss: 0.486144\n",
      "Training Step: 27900; average -> pos_sample_loss: 0.463207; neg_sample_loss: 0.509740; loss: 0.486473\n",
      "Training Step: 28000; average -> pos_sample_loss: 0.464366; neg_sample_loss: 0.507412; loss: 0.485889\n",
      "Training Step: 28100; average -> pos_sample_loss: 0.465153; neg_sample_loss: 0.509267; loss: 0.487210\n",
      "Training Step: 28200; average -> pos_sample_loss: 0.466535; neg_sample_loss: 0.507887; loss: 0.487211\n",
      "Training Step: 28300; average -> pos_sample_loss: 0.463879; neg_sample_loss: 0.508289; loss: 0.486084\n",
      "Training Step: 28400; average -> pos_sample_loss: 0.465494; neg_sample_loss: 0.507627; loss: 0.486561\n",
      "Training Step: 28500; average -> pos_sample_loss: 0.469700; neg_sample_loss: 0.505834; loss: 0.487767\n",
      "Training Step: 28600; average -> pos_sample_loss: 0.465826; neg_sample_loss: 0.509329; loss: 0.487577\n",
      "Training Step: 28700; average -> pos_sample_loss: 0.467563; neg_sample_loss: 0.508092; loss: 0.487828\n",
      "Training Step: 28800; average -> pos_sample_loss: 0.469431; neg_sample_loss: 0.502899; loss: 0.486165\n",
      "Training Step: 28900; average -> pos_sample_loss: 0.465431; neg_sample_loss: 0.503833; loss: 0.484632\n",
      "Training Step: 29000; average -> pos_sample_loss: 0.467620; neg_sample_loss: 0.505977; loss: 0.486798\n",
      "Training Step: 29100; average -> pos_sample_loss: 0.470025; neg_sample_loss: 0.503960; loss: 0.486993\n",
      "Training Step: 29200; average -> pos_sample_loss: 0.471653; neg_sample_loss: 0.502600; loss: 0.487126\n",
      "Training Step: 29300; average -> pos_sample_loss: 0.472924; neg_sample_loss: 0.505187; loss: 0.489055\n",
      "Training Step: 29400; average -> pos_sample_loss: 0.471967; neg_sample_loss: 0.504896; loss: 0.488431\n",
      "Training Step: 29500; average -> pos_sample_loss: 0.472432; neg_sample_loss: 0.502908; loss: 0.487670\n",
      "Training Step: 29600; average -> pos_sample_loss: 0.469634; neg_sample_loss: 0.501894; loss: 0.485764\n",
      "Training Step: 29700; average -> pos_sample_loss: 0.474019; neg_sample_loss: 0.500920; loss: 0.487469\n",
      "Training Step: 29800; average -> pos_sample_loss: 0.471202; neg_sample_loss: 0.501655; loss: 0.486429\n",
      "Training Step: 29900; average -> pos_sample_loss: 0.469603; neg_sample_loss: 0.503138; loss: 0.486370\n",
      "Training Step: 30000; average -> pos_sample_loss: 0.470513; neg_sample_loss: 0.503044; loss: 0.486778\n",
      "Training Step: 30100; average -> pos_sample_loss: 0.473456; neg_sample_loss: 0.499338; loss: 0.486397\n",
      "Training Step: 30200; average -> pos_sample_loss: 0.471044; neg_sample_loss: 0.501932; loss: 0.486488\n",
      "Training Step: 30300; average -> pos_sample_loss: 0.472514; neg_sample_loss: 0.498844; loss: 0.485679\n",
      "Training Step: 30400; average -> pos_sample_loss: 0.472611; neg_sample_loss: 0.496374; loss: 0.484493\n",
      "Training Step: 30500; average -> pos_sample_loss: 0.471183; neg_sample_loss: 0.501457; loss: 0.486320\n",
      "Training Step: 30600; average -> pos_sample_loss: 0.471284; neg_sample_loss: 0.497985; loss: 0.484634\n",
      "Training Step: 30700; average -> pos_sample_loss: 0.473128; neg_sample_loss: 0.497986; loss: 0.485557\n",
      "Training Step: 30800; average -> pos_sample_loss: 0.476291; neg_sample_loss: 0.499542; loss: 0.487917\n",
      "Training Step: 30900; average -> pos_sample_loss: 0.470848; neg_sample_loss: 0.498439; loss: 0.484644\n",
      "Training Step: 31000; average -> pos_sample_loss: 0.473405; neg_sample_loss: 0.495510; loss: 0.484458\n",
      "Training Step: 31100; average -> pos_sample_loss: 0.473571; neg_sample_loss: 0.496855; loss: 0.485213\n",
      "Training Step: 31200; average -> pos_sample_loss: 0.474638; neg_sample_loss: 0.495178; loss: 0.484908\n",
      "Training Step: 31300; average -> pos_sample_loss: 0.472919; neg_sample_loss: 0.498302; loss: 0.485611\n",
      "Training Step: 31400; average -> pos_sample_loss: 0.467054; neg_sample_loss: 0.493726; loss: 0.480390\n",
      "Training Step: 31500; average -> pos_sample_loss: 0.425349; neg_sample_loss: 0.493831; loss: 0.459590\n",
      "Training Step: 31600; average -> pos_sample_loss: 0.433414; neg_sample_loss: 0.487141; loss: 0.460277\n",
      "Training Step: 31700; average -> pos_sample_loss: 0.434825; neg_sample_loss: 0.486943; loss: 0.460884\n",
      "Training Step: 31800; average -> pos_sample_loss: 0.435264; neg_sample_loss: 0.485545; loss: 0.460405\n",
      "Training Step: 31900; average -> pos_sample_loss: 0.435711; neg_sample_loss: 0.483188; loss: 0.459449\n",
      "Training Step: 32000; average -> pos_sample_loss: 0.439241; neg_sample_loss: 0.485308; loss: 0.462274\n",
      "Training Step: 32100; average -> pos_sample_loss: 0.437842; neg_sample_loss: 0.482687; loss: 0.460264\n",
      "Training Step: 32200; average -> pos_sample_loss: 0.436238; neg_sample_loss: 0.481093; loss: 0.458665\n",
      "Training Step: 32300; average -> pos_sample_loss: 0.437895; neg_sample_loss: 0.482698; loss: 0.460296\n",
      "Training Step: 32400; average -> pos_sample_loss: 0.438701; neg_sample_loss: 0.483796; loss: 0.461248\n",
      "Training Step: 32500; average -> pos_sample_loss: 0.442003; neg_sample_loss: 0.478906; loss: 0.460454\n",
      "Training Step: 32600; average -> pos_sample_loss: 0.443286; neg_sample_loss: 0.480930; loss: 0.462108\n",
      "Training Step: 32700; average -> pos_sample_loss: 0.442203; neg_sample_loss: 0.481461; loss: 0.461832\n",
      "Training Step: 32800; average -> pos_sample_loss: 0.440829; neg_sample_loss: 0.478838; loss: 0.459833\n",
      "Training Step: 32900; average -> pos_sample_loss: 0.442604; neg_sample_loss: 0.479449; loss: 0.461026\n",
      "Training Step: 33000; average -> pos_sample_loss: 0.443363; neg_sample_loss: 0.482169; loss: 0.462766\n",
      "Training Step: 33100; average -> pos_sample_loss: 0.443745; neg_sample_loss: 0.481435; loss: 0.462590\n",
      "Training Step: 33200; average -> pos_sample_loss: 0.444981; neg_sample_loss: 0.478785; loss: 0.461883\n",
      "Training Step: 33300; average -> pos_sample_loss: 0.445191; neg_sample_loss: 0.477645; loss: 0.461418\n",
      "Training Step: 33400; average -> pos_sample_loss: 0.444047; neg_sample_loss: 0.477886; loss: 0.460966\n",
      "Training Step: 33500; average -> pos_sample_loss: 0.444500; neg_sample_loss: 0.475991; loss: 0.460245\n",
      "Training Step: 33600; average -> pos_sample_loss: 0.446407; neg_sample_loss: 0.476917; loss: 0.461662\n",
      "Training Step: 33700; average -> pos_sample_loss: 0.446029; neg_sample_loss: 0.478645; loss: 0.462337\n",
      "Training Step: 33800; average -> pos_sample_loss: 0.447659; neg_sample_loss: 0.476867; loss: 0.462263\n",
      "Training Step: 33900; average -> pos_sample_loss: 0.447366; neg_sample_loss: 0.475466; loss: 0.461416\n",
      "Training Step: 34000; average -> pos_sample_loss: 0.449883; neg_sample_loss: 0.475224; loss: 0.462554\n",
      "Training Step: 34100; average -> pos_sample_loss: 0.447493; neg_sample_loss: 0.473241; loss: 0.460367\n",
      "Training Step: 34200; average -> pos_sample_loss: 0.447310; neg_sample_loss: 0.475794; loss: 0.461552\n",
      "Training Step: 34300; average -> pos_sample_loss: 0.448238; neg_sample_loss: 0.473336; loss: 0.460787\n",
      "Training Step: 34400; average -> pos_sample_loss: 0.447087; neg_sample_loss: 0.475177; loss: 0.461132\n",
      "Training Step: 34500; average -> pos_sample_loss: 0.449770; neg_sample_loss: 0.475612; loss: 0.462691\n",
      "Training Step: 34600; average -> pos_sample_loss: 0.449429; neg_sample_loss: 0.472352; loss: 0.460891\n",
      "Training Step: 34700; average -> pos_sample_loss: 0.448388; neg_sample_loss: 0.471024; loss: 0.459706\n",
      "Training Step: 34800; average -> pos_sample_loss: 0.449746; neg_sample_loss: 0.473072; loss: 0.461409\n",
      "Training Step: 34900; average -> pos_sample_loss: 0.449998; neg_sample_loss: 0.472596; loss: 0.461297\n",
      "Training Step: 35000; average -> pos_sample_loss: 0.450909; neg_sample_loss: 0.471748; loss: 0.461329\n",
      "Training Step: 35100; average -> pos_sample_loss: 0.450354; neg_sample_loss: 0.470344; loss: 0.460349\n",
      "Training Step: 35200; average -> pos_sample_loss: 0.451174; neg_sample_loss: 0.471157; loss: 0.461165\n",
      "Training Step: 35300; average -> pos_sample_loss: 0.451024; neg_sample_loss: 0.470608; loss: 0.460816\n",
      "Training Step: 35400; average -> pos_sample_loss: 0.452044; neg_sample_loss: 0.473999; loss: 0.463021\n",
      "Training Step: 35500; average -> pos_sample_loss: 0.451584; neg_sample_loss: 0.471298; loss: 0.461441\n",
      "Training Step: 35600; average -> pos_sample_loss: 0.453712; neg_sample_loss: 0.471259; loss: 0.462485\n",
      "Training Step: 35700; average -> pos_sample_loss: 0.451018; neg_sample_loss: 0.470696; loss: 0.460857\n",
      "Training Step: 35800; average -> pos_sample_loss: 0.450183; neg_sample_loss: 0.470501; loss: 0.460342\n",
      "Training Step: 35900; average -> pos_sample_loss: 0.436582; neg_sample_loss: 0.469140; loss: 0.452861\n",
      "Training Step: 36000; average -> pos_sample_loss: 0.408321; neg_sample_loss: 0.465067; loss: 0.436694\n",
      "Training Step: 36100; average -> pos_sample_loss: 0.412633; neg_sample_loss: 0.464074; loss: 0.438354\n",
      "Training Step: 36200; average -> pos_sample_loss: 0.414397; neg_sample_loss: 0.458927; loss: 0.436662\n",
      "Training Step: 36300; average -> pos_sample_loss: 0.415087; neg_sample_loss: 0.459230; loss: 0.437159\n",
      "Training Step: 36400; average -> pos_sample_loss: 0.417931; neg_sample_loss: 0.458859; loss: 0.438395\n",
      "Training Step: 36500; average -> pos_sample_loss: 0.418185; neg_sample_loss: 0.459013; loss: 0.438599\n",
      "Training Step: 36600; average -> pos_sample_loss: 0.416625; neg_sample_loss: 0.459720; loss: 0.438173\n",
      "Training Step: 36700; average -> pos_sample_loss: 0.421236; neg_sample_loss: 0.457999; loss: 0.439617\n",
      "Training Step: 36800; average -> pos_sample_loss: 0.418803; neg_sample_loss: 0.456379; loss: 0.437591\n",
      "Training Step: 36900; average -> pos_sample_loss: 0.417577; neg_sample_loss: 0.457856; loss: 0.437717\n",
      "Training Step: 37000; average -> pos_sample_loss: 0.418386; neg_sample_loss: 0.454688; loss: 0.436537\n",
      "Training Step: 37100; average -> pos_sample_loss: 0.420540; neg_sample_loss: 0.455491; loss: 0.438016\n",
      "Training Step: 37200; average -> pos_sample_loss: 0.422809; neg_sample_loss: 0.454515; loss: 0.438662\n",
      "Training Step: 37300; average -> pos_sample_loss: 0.420642; neg_sample_loss: 0.453054; loss: 0.436848\n",
      "Training Step: 37400; average -> pos_sample_loss: 0.421753; neg_sample_loss: 0.456575; loss: 0.439164\n",
      "Training Step: 37500; average -> pos_sample_loss: 0.421569; neg_sample_loss: 0.453061; loss: 0.437315\n",
      "Training Step: 37600; average -> pos_sample_loss: 0.422882; neg_sample_loss: 0.452340; loss: 0.437611\n",
      "Training Step: 37700; average -> pos_sample_loss: 0.423243; neg_sample_loss: 0.452772; loss: 0.438007\n",
      "Training Step: 37800; average -> pos_sample_loss: 0.424754; neg_sample_loss: 0.451090; loss: 0.437922\n",
      "Training Step: 37900; average -> pos_sample_loss: 0.422316; neg_sample_loss: 0.452862; loss: 0.437589\n",
      "Training Step: 38000; average -> pos_sample_loss: 0.427891; neg_sample_loss: 0.455045; loss: 0.441468\n",
      "Training Step: 38100; average -> pos_sample_loss: 0.425017; neg_sample_loss: 0.453033; loss: 0.439025\n",
      "Training Step: 38200; average -> pos_sample_loss: 0.427547; neg_sample_loss: 0.450846; loss: 0.439196\n",
      "Training Step: 38300; average -> pos_sample_loss: 0.424096; neg_sample_loss: 0.447260; loss: 0.435678\n",
      "Training Step: 38400; average -> pos_sample_loss: 0.427761; neg_sample_loss: 0.452369; loss: 0.440065\n",
      "Training Step: 38500; average -> pos_sample_loss: 0.428462; neg_sample_loss: 0.450734; loss: 0.439598\n",
      "Training Step: 38600; average -> pos_sample_loss: 0.427637; neg_sample_loss: 0.449791; loss: 0.438714\n",
      "Training Step: 38700; average -> pos_sample_loss: 0.426935; neg_sample_loss: 0.448788; loss: 0.437861\n",
      "Training Step: 38800; average -> pos_sample_loss: 0.427946; neg_sample_loss: 0.451656; loss: 0.439801\n",
      "Training Step: 38900; average -> pos_sample_loss: 0.428178; neg_sample_loss: 0.447873; loss: 0.438025\n",
      "Training Step: 39000; average -> pos_sample_loss: 0.429583; neg_sample_loss: 0.448466; loss: 0.439025\n",
      "Training Step: 39100; average -> pos_sample_loss: 0.428964; neg_sample_loss: 0.448713; loss: 0.438839\n",
      "Training Step: 39200; average -> pos_sample_loss: 0.430043; neg_sample_loss: 0.449123; loss: 0.439583\n",
      "Training Step: 39300; average -> pos_sample_loss: 0.430746; neg_sample_loss: 0.448734; loss: 0.439740\n",
      "Training Step: 39400; average -> pos_sample_loss: 0.428041; neg_sample_loss: 0.446168; loss: 0.437104\n",
      "Training Step: 39500; average -> pos_sample_loss: 0.429327; neg_sample_loss: 0.448061; loss: 0.438694\n",
      "Training Step: 39600; average -> pos_sample_loss: 0.429583; neg_sample_loss: 0.446589; loss: 0.438086\n",
      "Training Step: 39700; average -> pos_sample_loss: 0.431669; neg_sample_loss: 0.448092; loss: 0.439880\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python3\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "\n",
    "def parse_args(args=None):\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Training and Testing Knowledge Graph Embedding Models',\n",
    "        usage='train.py [<args>] [-h | --help]'\n",
    "    )\n",
    "\n",
    "    parser.add_argument('--cuda', action='store_true', help='use GPU', default=False)\n",
    "    parser.add_argument('--CUDA_DEVISE', default='1', help='')\n",
    "\n",
    "    parser.add_argument('--do_train', action='store_true', default=True)\n",
    "    parser.add_argument('--do_valid', action='store_true')\n",
    "    parser.add_argument('--do_test', action='store_true')\n",
    "    parser.add_argument('--evaluate_train', action='store_true', help='Evaluate on training data', default=False)\n",
    "\n",
    "    parser.add_argument('--datadir', type=str, default='data')\n",
    "    parser.add_argument('--dataset', type=str, default='ontology')\n",
    "\n",
    "    parser.add_argument('-save', '--save_path', type=str)\n",
    "\n",
    "    parser.add_argument('--model', default='TransE', type=str)\n",
    "    parser.add_argument('-de', '--double_entity_embedding', action='store_true')\n",
    "    parser.add_argument('-dr', '--double_relation_embedding', action='store_true')\n",
    "\n",
    "    parser.add_argument('-n', '--negative_sample_size', default=1024, type=int)\n",
    "    parser.add_argument('-d', '--hidden_dim', default=100, type=int)\n",
    "    parser.add_argument('-g', '--gamma', default=12, type=float)\n",
    "    parser.add_argument('-adv', '--negative_adversarial_sampling', action='store_true', default=True)\n",
    "    parser.add_argument('-a', '--adversarial_temperature', default=1, type=float)\n",
    "    parser.add_argument('-b', '--batch_size', default=256, type=int)\n",
    "    parser.add_argument('-r', '--regularization', default=0.0, type=float)\n",
    "    parser.add_argument('--test_batch_size', default=8, type=int, help='valid/test batch size')\n",
    "    parser.add_argument('--uni_weight', action='store_true',\n",
    "                        help='Otherwise use subsampling weighting like in word2vec')\n",
    "\n",
    "    parser.add_argument('-lr', '--learning_rate', default=0.00005, type=float)\n",
    "    parser.add_argument('-cpu', '--cpu_num', default=10, type=int)\n",
    "\n",
    "\n",
    "    parser.add_argument('--max_steps', default=80000, type=int)\n",
    "    parser.add_argument('--warm_up_steps', default=None, type=int)\n",
    "\n",
    "    parser.add_argument('--save_steps', default=1000, type=int)\n",
    "    parser.add_argument('--valid_steps', default=1000, type=int)\n",
    "    parser.add_argument('--print_steps', default=100, type=int, help='train log every xx steps')\n",
    "    parser.add_argument('--test_log_steps', default=1000, type=int, help='valid/test log every xx steps')\n",
    "\n",
    "    parser.add_argument('--nentity', type=int, default=0, help='DO NOT MANUALLY SET')\n",
    "    parser.add_argument('--nrelation', type=int, default=0, help='DO NOT MANUALLY SET')\n",
    "\n",
    "    return parser.parse_args(args)\n",
    "\n",
    "\n",
    "def save_embeddings(model, step, args):\n",
    "    '''\n",
    "    Save the parameters of the model and the optimizer,\n",
    "    as well as some other variables such as step and learning_rate\n",
    "    '''\n",
    "    file_name = 'entity_' + str(step)\n",
    "    entity_embedding = model.entity_embedding.detach().cpu().numpy()\n",
    "\n",
    "    np.save(\n",
    "        os.path.join(args.save_path, file_name),\n",
    "        entity_embedding\n",
    "    )\n",
    "\n",
    "    rel_file_name = 'relation_' + str(step)\n",
    "    relation_embedding = model.relation_embedding.detach().cpu().numpy()\n",
    "    np.save(\n",
    "        os.path.join(args.save_path, rel_file_name),\n",
    "        relation_embedding\n",
    "    )\n",
    "\n",
    "\n",
    "def read_triple(file_path, entity2id, relation2id):\n",
    "    '''\n",
    "    Read triples and map them into ids.\n",
    "    '''\n",
    "    triples = []\n",
    "    with open(file_path) as fin:\n",
    "        for line in fin:\n",
    "            h, r, t = line.strip().split('\\t')\n",
    "            triples.append((entity2id[h], relation2id[r], entity2id[t]))\n",
    "    return triples\n",
    "\n",
    "\n",
    "def log_metrics(mode, step, metrics):\n",
    "    '''\n",
    "    Print the evaluation logs\n",
    "    '''\n",
    "    for metric in metrics:\n",
    "        print('%s %s at step %d: %f' % (mode, metric, step, metrics[metric]))\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    #os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.CUDA_DEVISE\n",
    "\n",
    "    args.data_path = os.path.join(args.datadir, args.dataset)\n",
    "\n",
    "\n",
    "    # if args.init_checkpoint:\n",
    "    #     override_config(args)\n",
    "    if args.data_path is None:\n",
    "        raise ValueError('data_path and dataset must be choosed.')\n",
    "\n",
    "    args.save_path = os.path.join(args.data_path, 'save_onto_embeds')\n",
    "\n",
    "    # if args.do_train and args.save_path is None:\n",
    "    #     raise ValueError('Where do you want to save your trained model?')\n",
    "\n",
    "    if args.save_path and not os.path.exists(args.save_path):\n",
    "        os.makedirs(args.save_path)\n",
    "\n",
    "    with open(os.path.join(args.data_path, 'entities.dict')) as fin:\n",
    "        entity2id = dict()\n",
    "        for line in fin:\n",
    "            eid, entity = line.strip().split('\\t')\n",
    "            entity2id[entity] = int(eid)\n",
    "\n",
    "    with open(os.path.join(args.data_path, 'relations.dict')) as fin:\n",
    "        relation2id = dict()\n",
    "        for line in fin:\n",
    "            rid, relation = line.strip().split('\\t')\n",
    "            relation2id[relation] = int(rid)\n",
    "\n",
    "    nentity = len(entity2id)\n",
    "    nrelation = len(relation2id)\n",
    "\n",
    "    args.nentity = nentity\n",
    "    args.nrelation = nrelation\n",
    "\n",
    "    print('Model: %s' % args.model)\n",
    "    # print('Data Path: %s' % args.data_path + \"/\" + args.dataset)\n",
    "    print('#entity num: %d' % nentity)\n",
    "    print('#relation num: %d' % nrelation)\n",
    "\n",
    "    all_triples = read_triple(os.path.join(args.data_path, 'triples.txt'), entity2id,\n",
    "                              relation2id)\n",
    "    print('#total triples num: %d' % len(all_triples))\n",
    "\n",
    "\n",
    "    # All true triples\n",
    "    all_true_triples = all_triples\n",
    "\n",
    "    kge_model = KGEModel(\n",
    "        model_name=args.model,\n",
    "        nentity=nentity,\n",
    "        nrelation=nrelation,\n",
    "        hidden_dim=args.hidden_dim,\n",
    "        gamma=args.gamma,\n",
    "        double_entity_embedding=args.double_entity_embedding,\n",
    "        double_relation_embedding=args.double_relation_embedding\n",
    "    )\n",
    "\n",
    "    # logging.info('Model Parameter Configuration:')\n",
    "    # for name, param in kge_model.named_parameters():\n",
    "    #     logging.info('Parameter %s: %s, require_grad = %s' % (name, str(param.size()), str(param.requires_grad)))\n",
    "\n",
    "    #if args.cuda:\n",
    "    #    kge_model = kge_model.cuda()\n",
    "\n",
    "    if args.do_train:\n",
    "        # Set training dataloader iterator\n",
    "        train_dataloader_head = DataLoader(\n",
    "            TrainDataset(all_triples, nentity, nrelation, args.negative_sample_size, 'head-batch'),\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=max(1, args.cpu_num // 2),\n",
    "            collate_fn=TrainDataset.collate_fn\n",
    "        )\n",
    "\n",
    "        train_dataloader_tail = DataLoader(\n",
    "            TrainDataset(all_triples, nentity, nrelation, args.negative_sample_size, 'tail-batch'),\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=max(1, args.cpu_num // 2),\n",
    "            collate_fn=TrainDataset.collate_fn\n",
    "        )\n",
    "\n",
    "        train_iterator = BidirectionalOneShotIterator(train_dataloader_head, train_dataloader_tail)\n",
    "\n",
    "        # Set training configuration\n",
    "        current_learning_rate = args.learning_rate\n",
    "\n",
    "        optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, kge_model.parameters()),\n",
    "            lr=current_learning_rate\n",
    "        )\n",
    "        if args.warm_up_steps:\n",
    "            warm_up_steps = args.warm_up_steps\n",
    "        else:\n",
    "            warm_up_steps = args.max_steps // 2\n",
    "\n",
    "    print('Ramdomly Initializing %s Model...' % args.model)\n",
    "\n",
    "    # step = init_step\n",
    "\n",
    "    print('------ Start Training...')\n",
    "    print('batch_size = %d' % args.batch_size)\n",
    "    print('negative sample size = %d' % args.negative_sample_size)\n",
    "    print('hidden_dim = %d' % args.hidden_dim)\n",
    "    print('gamma = %f' % args.gamma)\n",
    "    print('negative_adversarial_sampling = %s' % str(args.negative_adversarial_sampling))\n",
    "\n",
    "    if args.negative_adversarial_sampling:\n",
    "        print('adversarial_temperature = %f' % args.adversarial_temperature)\n",
    "\n",
    "    print(\"learning rate = %f\" % current_learning_rate)\n",
    "\n",
    "    # Set valid dataloader as it would be evaluated during training\n",
    "\n",
    "    if args.do_train:\n",
    "\n",
    "        train_losses = []\n",
    "\n",
    "        # Training Loop\n",
    "        for step in range(1, args.max_steps + 1):\n",
    "\n",
    "            loss_values = kge_model.train_step(kge_model, optimizer, train_iterator, args)\n",
    "\n",
    "            train_losses.append(loss_values)\n",
    "\n",
    "            if step >= warm_up_steps:\n",
    "                current_learning_rate = current_learning_rate / 10\n",
    "                print('Change learning_rate to %f at step %d' % (current_learning_rate, step))\n",
    "                optimizer = torch.optim.Adam(\n",
    "                    filter(lambda p: p.requires_grad, kge_model.parameters()),\n",
    "                    lr=current_learning_rate\n",
    "                )\n",
    "                warm_up_steps = warm_up_steps * 3\n",
    "\n",
    "            if step % args.print_steps == 0:\n",
    "                pos_sample_loss = sum([losses['pos_sample_loss'] for losses in train_losses]) / len(train_losses)\n",
    "                neg_sample_loss = sum([losses['neg_sample_loss'] for losses in train_losses]) / len(train_losses)\n",
    "                loss1 = sum([losses['loss'] for losses in train_losses]) / len(train_losses)\n",
    "\n",
    "                # log_metrics('Training average', step, metrics)\n",
    "                print('Training Step: %d; average -> pos_sample_loss: %f; neg_sample_loss: %f; loss: %f' %\n",
    "                      (step, pos_sample_loss, neg_sample_loss, loss1))\n",
    "                train_losses = []\n",
    "\n",
    "            if step % args.save_steps == 0:\n",
    "                save_embeddings(kge_model, step, args)\n",
    "\n",
    "            if args.evaluate_train and step % args.valid_steps == 0:\n",
    "                print('------ Evaluating on Training Dataset...')\n",
    "                metrics = kge_model.test_step(kge_model, all_triples, all_true_triples, args)\n",
    "                log_metrics('Test', step, metrics)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # random_seed = random.randint(1, 10000)\n",
    "    # random_seed = 5487\n",
    "    #\n",
    "    # print(\"random seed:\", random_seed)\n",
    "    # random.seed(random_seed)\n",
    "    # torch.manual_seed(random_seed)\n",
    "    # torch.cuda.manual_seed_all(random_seed)\n",
    "    main(parse_args(args=['--max_steps', '1000000', '--save_steps', '10000', '--valid_steps', '1000000', '--datadir', '../persistent/data']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff777ee-b398-4126-9d8f-45cc68d18561",
   "metadata": {},
   "source": [
    "### Combining the entities and relations embeddings into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6edc72c-b849-464a-9aaf-8221519f0d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124749\n",
      "5\n",
      "(124749, 100)\n",
      "(5, 100)\n",
      "124597\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import codecs\n",
    "import argparse\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "def parse_args(args=None):\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Data pre-process',\n",
    "        usage=''\n",
    "    )\n",
    "    parser.add_argument('--data_dir', type=str, default='data')\n",
    "    parser.add_argument('--onto_dir', type=str, default='ontology')\n",
    "    parser.add_argument('--glove_dir', type=str, default='glove')\n",
    "    parser.add_argument('--struct_embeds_fn', default='Onto_TransE.pkl', help='')\n",
    "    parser.add_argument('--word_embeds_fn', default='Onto_Text_Embed.pkl', help='')\n",
    "    parser.add_argument('--triples_fn', default='triples_names_htr.txt', help='')\n",
    "    parser.add_argument('--entities_fn', default='entities.dict', help='')\n",
    "    parser.add_argument('--entities_names_fn', default='entities_names.dict', help='')\n",
    "    parser.add_argument('--entities_embed_fn', default='entity_500.npy', help='entities embedding filename')\n",
    "    parser.add_argument('--relations_embed_fn', default='relation_500.npy', help='relations embedding filename')\n",
    "    parser.add_argument('--struct_embed_size', default=100, type=int, help='entity structural embeddings size')\n",
    "    parser.add_argument('--text_embed_size', default=300, type=int, help='entity textual embeddings size')\n",
    "    parser.add_argument('--mapping_size', default=100, type=int, help='hidden layer size')\n",
    "    parser.add_argument('--dropout_ratio', default=0.5, type=float, help='')\n",
    "    parser.add_argument('--margin', default=10, type=int, help='')\n",
    "    parser.add_argument('--training_epochs', default=10, type=int, help='')\n",
    "    parser.add_argument('--batch_size', default=100, type=int, help='')\n",
    "    parser.add_argument('--display_loss_step', default=1000, type=int, help='')\n",
    "    parser.add_argument('--initial_learning_rate', default=0.001, type=float, help='')\n",
    "    parser.add_argument('--activation_function', default='', help='')\n",
    "    parser.add_argument('--warm_up_steps', default=0, type=int)\n",
    "\n",
    "    return parser.parse_args(args)\n",
    "\n",
    "\n",
    "# set the path to your data folders here\n",
    "param = parse_args(args=['--data_dir', '../persistent/data',\n",
    "                         '--entities_embed_fn', 'entity_500000.npy',\n",
    "                         '--relations_embed_fn', 'relation_500000.npy'])\n",
    "\n",
    "onto_dir_path = os.path.join(param.data_dir, param.onto_dir)\n",
    "\n",
    "def loadDict(file_name):\n",
    "    entities = list()\n",
    "    wnids = open(file_name, 'r')\n",
    "    try:\n",
    "        for line in wnids:\n",
    "            line = line[:-1]\n",
    "            index, cls = line.split('\\t')\n",
    "            entities.append(cls)\n",
    "    finally:\n",
    "        wnids.close()\n",
    "    print(len(entities))\n",
    "    return entities\n",
    "\n",
    "\n",
    "entity_file = os.path.join(onto_dir_path, 'entities_names.dict')\n",
    "relation_file = os.path.join(onto_dir_path, 'relations.dict')\n",
    "\n",
    "\n",
    "# load entity dict\n",
    "entities = loadDict(entity_file)\n",
    "relations = loadDict(relation_file)\n",
    "\n",
    "embed_dir = os.path.join(onto_dir_path, 'save_onto_embeds')\n",
    "\n",
    "ent_embed_file = os.path.join(embed_dir, param.entities_embed_fn)\n",
    "rel_embed_file = os.path.join(embed_dir, param.relations_embed_fn)\n",
    "\n",
    "ent_embeds = np.load(ent_embed_file)\n",
    "print(ent_embeds.shape)\n",
    "\n",
    "rel_embeds = np.load(rel_embed_file)\n",
    "print(rel_embeds.shape)\n",
    "\n",
    "embed_dict = dict()\n",
    "for i, ent in enumerate(entities):\n",
    "    embed_dict[ent] = ent_embeds[i].astype('float32')\n",
    "for i, rel in enumerate(relations):\n",
    "    embed_dict[rel] = rel_embeds[i].astype('float32')\n",
    "\n",
    "print(len(embed_dict.keys()))\n",
    "\n",
    "\n",
    "\n",
    "embeddings_path = os.path.join(onto_dir_path, 'embeddings')\n",
    "if not os.path.exists(embeddings_path):\n",
    "    os.makedirs(embeddings_path)\n",
    "\n",
    "with open(os.path.join(onto_dir_path, 'embeddings', 'Onto_TransE.pkl'), 'wb') as f:\n",
    "    pkl.dump(embed_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
